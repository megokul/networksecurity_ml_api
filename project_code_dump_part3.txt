
ðŸ“¦ Project Structure of: E:\MyProjects\networksecurity

ðŸ“ .dvc/
    ðŸ“„ .gitignore
    ðŸ“ cache/
        ðŸ“ files/
            ðŸ“ md5/
                ðŸ“ 68/
                    ðŸ“„ 770e0efceb9cd3777502f505ecbec4
                ðŸ“ 70/
                    ðŸ“„ 3506878baa6bf0360b2b42782e9677
                ðŸ“ 83/
                    ðŸ“„ de4e3a0dbe7d93403a8c15753377f7
                ðŸ“ 86/
                    ðŸ“„ 4e5c8474fcd1b15bc434932036628d
                ðŸ“ 87/
                    ðŸ“„ f220430be73d16609bfd637b1644af
                ðŸ“ 8d/
                    ðŸ“„ e71e17a20f71bece7659a34bae7027
                ðŸ“ bf/
                    ðŸ“„ fa4350e8aa16caaa3cbb92ec8e048c
                ðŸ“ e1/
                    ðŸ“„ a8ae356f072436dd3acc1bcbafbd89
    ðŸ“„ config
    ðŸ“ tmp/
        ðŸ“„ btime
        ðŸ“„ dag.md
        ðŸ“ exps/
            ðŸ“ cache/
                ðŸ“ 08/
                    ðŸ“„ ba3af380fc7743bb7784b0b99b9e84b597659c
                ðŸ“ 2e/
                    ðŸ“„ d0f21b70c06ff15b188bb69f6ce389a869af78
                ðŸ“ 32/
                    ðŸ“„ 9b746e5971f793e269ced861c2d1e401f43d48
                ðŸ“ 3e/
                    ðŸ“„ a84c7b28b0242174e45334d5d6a9d3026f546d
                ðŸ“ 48/
                    ðŸ“„ faf510268751ce441d9178a563f5e2c4577d88
                ðŸ“ 4a/
                    ðŸ“„ 640ce33b51ffeb8c1aa85e520ce78dfe105123
                ðŸ“ 61/
                    ðŸ“„ a81d71c590a7a662176a82aa5f00a7532a3e16
                ðŸ“ 8a/
                    ðŸ“„ ed08439cb37bbee9fee4ad00d82ea30435606b
                ðŸ“ a8/
                    ðŸ“„ 2c1cb5bbc048ca32cb78061a4b8aaf3735233f
                ðŸ“ b0/
                    ðŸ“„ 63569211a95f11f23c4f61f6b9518bbf3b2cf3
                ðŸ“ bb/
                    ðŸ“„ 6504b5b79c287b4b60d2355251d8b30025c769
                ðŸ“ c4/
                    ðŸ“„ 9033287ca54d77ad6ab9c7603c06825bbac4e6
                ðŸ“ c7/
                    ðŸ“„ 31a093cb23b7a2bef65922d39d9d20ac56e6f3
                ðŸ“ e3/
                    ðŸ“„ 5de325a2125360b5ae289c080fc748a0140cdc
                ðŸ“ eb/
                    ðŸ“„ 79e8cfe8c4a974d876650c1a48de56761ef2a1
                ðŸ“ f9/
                    ðŸ“„ 21cb7aa3662a13f16a67e07a71781cec9b1a91
            ðŸ“ celery/
                ðŸ“ broker/
                    ðŸ“ control/
                    ðŸ“ in/
                    ðŸ“ processed/
                ðŸ“ result/
        ðŸ“„ lock
        ðŸ“„ rwlock
        ðŸ“„ rwlock.lock
ðŸ“„ .dvcignore
ðŸ“„ .env
ðŸ“„ .gitignore
ðŸ“„ Dockerfile
ðŸ“„ LICENSE
ðŸ“ NetworkSecurity.egg-info/
    ðŸ“„ PKG-INFO
    ðŸ“„ SOURCES.txt
    ðŸ“„ dependency_links.txt
    ðŸ“„ requires.txt
    ðŸ“„ top_level.txt
ðŸ“„ README.md
ðŸ“„ app.py
ðŸ“ artifacts/
    ðŸ“ 2025_05_01T19_59_20Z/
        ðŸ“ data_ingestion/
            ðŸ“ featurestore/
                ðŸ“„ raw_data.csv
            ðŸ“ ingested/
                ðŸ“„ ingested_data.csv
        ðŸ“ data_transformation/
            ðŸ“ preprocessor/
                ðŸ“„ x_preprocessor.joblib
                ðŸ“„ y_preprocessor.joblib
            ðŸ“ transformed/
                ðŸ“ test/
                    ðŸ“„ x_test.npy
                    ðŸ“„ y_test.npy
                ðŸ“ train/
                    ðŸ“„ x_train.npy
                    ðŸ“„ y_train.npy
                ðŸ“ val/
                    ðŸ“„ x_val.npy
                    ðŸ“„ y_val.npy
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.json
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
        ðŸ“ model_evaluation/
            ðŸ“„ evaluation_report.yaml
        ðŸ“ model_trainer/
            ðŸ“ inference_model/
                ðŸ“„ inference_model.joblib
            ðŸ“ reports/
                ðŸ“„ training_report.yaml
            ðŸ“ trained_model/
                ðŸ“„ model.joblib
    ðŸ“ 2025_05_01T20_31_11Z/
        ðŸ“ data_ingestion/
            ðŸ“ featurestore/
                ðŸ“„ raw_data.csv
            ðŸ“ ingested/
                ðŸ“„ ingested_data.csv
        ðŸ“ data_transformation/
            ðŸ“ preprocessor/
                ðŸ“„ x_preprocessor.joblib
                ðŸ“„ y_preprocessor.joblib
            ðŸ“ transformed/
                ðŸ“ test/
                    ðŸ“„ x_test.npy
                    ðŸ“„ y_test.npy
                ðŸ“ train/
                    ðŸ“„ x_train.npy
                    ðŸ“„ y_train.npy
                ðŸ“ val/
                    ðŸ“„ x_val.npy
                    ðŸ“„ y_val.npy
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.json
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
        ðŸ“ model_evaluation/
            ðŸ“„ evaluation_report.yaml
        ðŸ“ model_trainer/
            ðŸ“ inference_model/
                ðŸ“„ inference_model.joblib
            ðŸ“ reports/
                ðŸ“„ training_report.yaml
            ðŸ“ trained_model/
                ðŸ“„ model.joblib
ðŸ“ config/
    ðŸ“„ config.yaml
    ðŸ“„ params.yaml
    ðŸ“„ schema.yaml
    ðŸ“„ templates.yaml
ðŸ“ data/
    ðŸ“ raw/
        ðŸ“„ raw_data.csv
        ðŸ“„ raw_data.csv.dvc
    ðŸ“ transformed/
        ðŸ“ test/
            ðŸ“„ x_test.npy
            ðŸ“„ x_test.npy.dvc
            ðŸ“„ y_test.npy
            ðŸ“„ y_test.npy.dvc
        ðŸ“ train/
            ðŸ“„ x_train.npy
            ðŸ“„ x_train.npy.dvc
            ðŸ“„ y_train.npy
            ðŸ“„ y_train.npy.dvc
        ðŸ“ val/
            ðŸ“„ x_val.npy
            ðŸ“„ x_val.npy.dvc
            ðŸ“„ y_val.npy
            ðŸ“„ y_val.npy.dvc
    ðŸ“ validated/
        ðŸ“„ validated_data.csv
        ðŸ“„ validated_data.csv.dvc
ðŸ“„ debug.py
ðŸ“„ debug_pipeline.py
ðŸ“ final_model/
    ðŸ“„ final_inference_model.joblib
ðŸ“ logs/
    ðŸ“ 2025_05_01T19_59_20Z/
        ðŸ“„ 2025_05_01T19_59_20Z.log
    ðŸ“ 2025_05_01T20_31_11Z/
        ðŸ“„ 2025_05_01T20_31_11Z.log
ðŸ“„ main.py
ðŸ“ network_data/
    ðŸ“ input_csv/
        ðŸ“„ phisingData.csv
ðŸ“„ notes.txt
ðŸ“„ print_structure.py
ðŸ“„ project_dump.py
ðŸ“„ project_template.py
ðŸ“„ prompt.txt
ðŸ“„ requirements.txt
ðŸ“ research/
    ðŸ“ config/
        ðŸ“„ schema.yaml
    ðŸ“„ ingested_data.csv
    ðŸ“ logs/
        ðŸ“ 2025_04_10T16_40_06/
            ðŸ“„ 2025_04_10T16_40_06.log
        ðŸ“ 2025_04_10T17_30_35/
            ðŸ“„ 2025_04_10T17_30_35.log
        ðŸ“ 2025_04_10T17_38_55/
            ðŸ“„ 2025_04_10T17_38_55.log
    ðŸ“„ research.ipynb
ðŸ“„ setup.py
ðŸ“ src/
    ðŸ“ networksecurity/
        ðŸ“„ __init__.py
        ðŸ“ cloud/
            ðŸ“„ __init__.py
            ðŸ“„ s3_syncer.py
        ðŸ“ components/
            ðŸ“„ __init__.py
            ðŸ“„ data_ingestion.py
            ðŸ“„ data_transformation.py
            ðŸ“„ data_validation.py
            ðŸ“„ model_evaluation.py
            ðŸ“„ model_pusher.py
            ðŸ“„ model_trainer.py
        ðŸ“ config/
            ðŸ“„ __init__.py
            ðŸ“„ configuration.py
        ðŸ“ constants/
            ðŸ“„ __init__.py
            ðŸ“„ constants.py
        ðŸ“ data_processors/
            ðŸ“„ encoder_factory.py
            ðŸ“„ imputer_factory.py
            ðŸ“„ label_mapper.py
            ðŸ“„ preprocessor_builder.py
            ðŸ“„ scaler_factory.py
        ðŸ“ dbhandler/
            ðŸ“„ __init__.py
            ðŸ“„ base_handler.py
            ðŸ“„ mongodb_handler.py
        ðŸ“ entity/
            ðŸ“„ __init__.py
            ðŸ“„ artifact_entity.py
            ðŸ“„ config_entity.py
        ðŸ“ exception/
            ðŸ“„ __init__.py
            ðŸ“„ exception.py
        ðŸ“ inference/
            ðŸ“„ estimator.py
        ðŸ“ logging/
            ðŸ“„ __init__.py
            ðŸ“„ logger.py
        ðŸ“ pipeline/
            ðŸ“„ __init__.py
            ðŸ“„ data_ingestion_pipeline.py
            ðŸ“„ data_transformation_pipeline.py
            ðŸ“„ data_validation_pipeline.py
            ðŸ“„ model_evaluation_pipeline.py
            ðŸ“„ model_pusher_pipeline.py
            ðŸ“„ model_trainer_pipeline.py
            ðŸ“„ training_pipeline.py
        ðŸ“ utils/
            ðŸ“„ __init__.py
            ðŸ“„ core.py
            ðŸ“„ timestamp.py
        ðŸ“ worker/
            ðŸ“„ celery_worker.py
ðŸ“ templates/
    ðŸ“„ table.html

--- CODE DUMP | PART 3 of 3 ---


================================================================================
# PY FILE: src\networksecurity\logging\__init__.py
================================================================================

"""
Initialize centralized logger for the `networksecurity.logging` package.

This module sets up a reusable logger instance (`logger`) that can be imported
across the project to ensure consistent, centralized logging configuration.

Supports:
- Shared UTC timestamp for file/folder naming
- Dynamic logger name via environment variable
- Dynamic log level via environment variable
"""

import os
import logging

from .logger import setup_logger

# Use env variable for logger name, default to "networksecurity"
LOGGER_NAME = os.getenv("LOGGER_NAME", "networksecurity")

# Use env variable for log level, default to "DEBUG"
LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG").upper()

# Initialize and configure the logger
logger = setup_logger(name=LOGGER_NAME)
logger.setLevel(getattr(logging, LOG_LEVEL, logging.DEBUG))

================================================================================
# PY FILE: src\networksecurity\logging\logger.py
================================================================================

"""
Logging utility module.

Provides `setup_logger()` to configure a logger with both a file and stream handler,
using a UTC timestamp synchronized across the pipeline (logs, artifacts, models, etc.).
"""

import logging
import sys
from pathlib import Path

from src.networksecurity.constants.constants import LOGS_ROOT
from src.networksecurity.utils.timestamp import get_shared_utc_timestamp

def setup_logger(name: str = "app_logger") -> logging.Logger:
    """
    Set up and return a logger instance with a consistent timestamped log directory and file.

    Ensures:
    - One timestamp per pipeline run (shared with ConfigurationManager)
    - No duplicate handlers on repeated setup
    - Clean log formatting to stdout and file

    Args:
        name (str): The name of the logger instance (e.g., 'data_ingestion').

    Returns:
        logging.Logger: Configured logger with file and stream handlers.
    """
    sys.stdout.reconfigure(encoding='utf-8')

    # Get shared timestamp for this run
    timestamp = get_shared_utc_timestamp()

    # Log folder: logs/<timestamp>/
    log_dir = Path(LOGS_ROOT) / timestamp
    log_dir.mkdir(parents=True, exist_ok=True)

    # Log file: logs/<timestamp>/<timestamp>.log
    log_filepath = log_dir / f"{timestamp}.log"

    # Log message format
    log_format = "[%(asctime)s] - %(levelname)s - %(module)s - %(message)s"
    formatter = logging.Formatter(log_format)

    # Get or create logger
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)

    # Add file handler if not already added
    if not any(isinstance(h, logging.FileHandler) and h.baseFilename == str(log_filepath)
               for h in logger.handlers):
        file_handler = logging.FileHandler(log_filepath, mode="a")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    # Add stdout stream handler if not already added
    if not any(isinstance(h, logging.StreamHandler) and h.stream == sys.stdout
               for h in logger.handlers):
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)

    return logger


# Example usage
if __name__ == "__main__":
    logger = setup_logger()
    logger.info("Logger initialized successfully.")

================================================================================
# PY FILE: src\networksecurity\pipeline\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\pipeline\data_ingestion_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_ingestion import DataIngestion
from src.networksecurity.dbhandler.mongodb_handler import MongoDBHandler
from src.networksecurity.entity.config_entity import DataIngestionConfig, MongoHandlerConfig
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataIngestionPipeline:
    """
    Runs the data ingestion stage:
    - Loads configs
    - Instantiates handler and component
    - Triggers ingestion and returns artifact
    """

    def __init__(self):
        try:
            self.config_manager = ConfigurationManager()
            self.ingestion_config: DataIngestionConfig = self.config_manager.get_data_ingestion_config()
            self.mongo_config: MongoHandlerConfig = self.config_manager.get_mongo_handler_config()
            self.mongo_handler = MongoDBHandler(config=self.mongo_config)
        except Exception as e:
            logger.exception("Failed to initialize DataIngestionPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> DataIngestionArtifact:
        try:
            logger.info("========== Data Ingestion Stage Started ==========")

            ingestion = DataIngestion(
                config=self.ingestion_config,
                db_handler=self.mongo_handler,
            )
            artifact = ingestion.run_ingestion()

            logger.info(f"Data Ingestion Process Completed.\n{artifact}")
            logger.info("========== Data Ingestion Stage Completed ==========")

            return artifact

        except Exception as e:
            logger.exception("Data Ingestion Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\data_transformation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_transformation import DataTransformation
from src.networksecurity.entity.artifact_entity import (
    DataValidationArtifact,
    DataTransformationArtifact,
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataTransformationPipeline:
    """
    Orchestrates the Data Transformation stage of the pipeline.

    Responsibilities:
    - Loads transformation configuration
    - Accepts validated artifact
    - Performs feature transformation and returns transformation artifact
    """

    def __init__(self, validation_artifact: DataValidationArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.config = self.config_manager.get_data_transformation_config()
            self.validation_artifact = validation_artifact
        except Exception as e:
            logger.exception("Failed to initialize DataTransformationPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> DataTransformationArtifact:
        try:
            logger.info("========== Data Transformation Stage Started ==========")

            transformer = DataTransformation(
                config=self.config,
                validation_artifact=self.validation_artifact,
            )
            transformation_artifact = transformer.run_transformation()

            logger.info(f"Data Transformation Completed Successfully: {transformation_artifact}")
            logger.info("========== Data Transformation Stage Completed ==========")

            return transformation_artifact

        except Exception as e:
            logger.exception("Data Transformation Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\data_validation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_validation import DataValidation
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataValidationPipeline:
    """
    Orchestrates the Data Validation stage of the pipeline.

    Responsibilities:
    - Fetches the configuration and artifacts
    - Loads validated DataFrame from artifact
    - Validates schema and schema hash
    """

    def __init__(self, ingestion_artifact: DataIngestionArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.ingestion_artifact = ingestion_artifact
            self.config = self.config_manager.get_data_validation_config()
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run(self):
        try:
            logger.info("========= Data Validation Stage Started =========")
            validation = DataValidation(config=self.config, ingestion_artifact=self.ingestion_artifact)
            validation_artifact = validation.run_validation()
            logger.info(f"Data Validation Process Completed.\n{validation_artifact}")
            logger.info("========= Data Validation Stage Completed =========")
            return validation_artifact
        except Exception as e:
            logger.error("Data Validation Pipeline Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\model_evaluation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_evaluation import ModelEvaluation
from src.networksecurity.entity.artifact_entity import (
    ModelTrainerArtifact,
    ModelEvaluationArtifact,
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError

class ModelEvaluationPipeline:
    """
    Orchestrates the Model Evaluation stage of the pipeline.

    Responsibilities:
    - Loads model evaluation configuration
    - Accepts only the trainer artifact (no transformation artifact needed)
    - Evaluates trained model on train/val/test datasets
    - Emits a ModelEvaluationArtifact
    """

    def __init__(self, trainer_artifact: ModelTrainerArtifact):
        try:
            logger.info("Initializing ModelEvaluationPipeline...")
            self.config = ConfigurationManager().get_model_evaluation_config()
            self.trainer_artifact = trainer_artifact
        except Exception as e:
            logger.exception("Failed to initialize ModelEvaluationPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelEvaluationArtifact:
        try:
            logger.info("========== Model Evaluation Stage Started ==========")

            evaluator = ModelEvaluation(
                config=self.config,
                trainer_artifact=self.trainer_artifact
            )
            evaluation_artifact = evaluator.run_evaluation()

            logger.info(f"Model Evaluation Completed Successfully: {evaluation_artifact}")
            logger.info("========== Model Evaluation Stage Completed ==========")

            return evaluation_artifact

        except Exception as e:
            logger.exception("Model Evaluation Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\model_pusher_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_pusher import ModelPusher
from src.networksecurity.entity.artifact_entity import ModelTrainerArtifact, ModelPusherArtifact
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class ModelPusherPipeline:
    """
    Orchestrates the model pushing stage.

    Responsibilities:
    - Loads model pusher configuration
    - Accepts the ModelTrainerArtifact
    - Saves final model to local path
    - Pushes model to S3 via S3Syncer
    - Emits a ModelPusherArtifact
    """

    def __init__(self, model_trainer_artifact: ModelTrainerArtifact) -> None:
        try:
            logger.info("Initializing ModelPusherPipeline...")

            self.config = ConfigurationManager().get_model_pusher_config()
            self.model_trainer_artifact = model_trainer_artifact

        except Exception as e:
            logger.exception("Failed to initialize ModelPusherPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelPusherArtifact:
        try:
            logger.info("========== Model Pusher Stage Started ==========")

            model_pusher = ModelPusher(
                model_pusher_config=self.config,
                model_trainer_artifact=self.model_trainer_artifact
            )
            pusher_artifact = model_pusher.push_model()

            logger.info(f"Model Pusher Stage Completed Successfully: {pusher_artifact}")
            logger.info("========== Model Pusher Stage Completed ==========")

            return pusher_artifact

        except Exception as e:
            logger.exception("Model Pusher Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\model_trainer_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_trainer import ModelTrainer
from src.networksecurity.entity.artifact_entity import (
    DataTransformationArtifact,
    ModelTrainerArtifact
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class ModelTrainerPipeline:
    """
    Orchestrates the Model Training stage of the pipeline.

    Responsibilities:
    - Loads modelâ€trainer configuration
    - Accepts transformation artifact
    - Trains (and tunes) candidate models
    - Logs & registers via MLflow
    - Emits a ModelTrainerArtifact
    """

    def __init__(self, transformation_artifact: DataTransformationArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.config = self.config_manager.get_model_trainer_config()
            self.transformation_artifact = transformation_artifact
        except Exception as e:
            logger.exception("Failed to initialize ModelTrainerPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelTrainerArtifact:
        try:
            logger.info("========== Model Training Stage Started ==========")

            trainer = ModelTrainer(
                config=self.config,
                transformation_artifact=self.transformation_artifact
            )
            trainer_artifact = trainer.run_training()

            logger.info(f"Model Training Completed Successfully: {trainer_artifact}")
            logger.info("========== Model Training Stage Completed ==========")

            return trainer_artifact

        except Exception as e:
            logger.exception("Model Training Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\training_pipeline.py
================================================================================

import sys

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger

from src.networksecurity.components.data_ingestion import DataIngestion
from src.networksecurity.components.data_validation import DataValidation
from src.networksecurity.components.data_transformation import DataTransformation
from src.networksecurity.components.model_trainer import ModelTrainer
from src.networksecurity.components.model_evaluation import ModelEvaluation
from src.networksecurity.components.model_pusher import ModelPusher

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.dbhandler.mongodb_handler import MongoDBHandler

from src.networksecurity.entity.artifact_entity import (
    DataIngestionArtifact,
    DataValidationArtifact,
    DataTransformationArtifact,
    ModelTrainerArtifact,
    ModelEvaluationArtifact,
    ModelPusherArtifact,
)


class TrainingPipeline:
    """
    Orchestrates the full end-to-end training pipeline:
    - MongoDB ingestion
    - Validation
    - Transformation
    - Training
    - Evaluation
    - Model push (local + optional S3)
    """

    def __init__(self) -> None:
        try:
            self.config_manager = ConfigurationManager()
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_pipeline(self) -> ModelPusherArtifact:
        try:
            logger.info("========== Training Pipeline Started ==========")

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # MongoDB Handler
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            mongo_config = self.config_manager.get_mongo_handler_config()
            mongo_handler = MongoDBHandler(config=mongo_config)

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Data Ingestion
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            ingestion_config = self.config_manager.get_data_ingestion_config()
            ingestion = DataIngestion(config=ingestion_config, db_handler=mongo_handler)
            ingestion_artifact: DataIngestionArtifact = ingestion.run_ingestion()

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Data Validation
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            validation_config = self.config_manager.get_data_validation_config()
            validation = DataValidation(config=validation_config, ingestion_artifact=ingestion_artifact)
            validation_artifact: DataValidationArtifact = validation.run_validation()

            if not validation_artifact.validation_status:
                logger.error("Validation failed. Aborting pipeline.")
                raise RuntimeError("Data validation failed. Pipeline terminated.")

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Data Transformation
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            transformation_config = self.config_manager.get_data_transformation_config()
            transformation = DataTransformation(config=transformation_config, validation_artifact=validation_artifact)
            transformation_artifact: DataTransformationArtifact = transformation.run_transformation()

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Model Training
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            trainer_config = self.config_manager.get_model_trainer_config()
            trainer = ModelTrainer(config=trainer_config, transformation_artifact=transformation_artifact)
            trainer_artifact: ModelTrainerArtifact = trainer.run_training()

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Model Evaluation
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            evaluation_config = self.config_manager.get_model_evaluation_config()
            evaluator = ModelEvaluation(config=evaluation_config, trainer_artifact=trainer_artifact)
            evaluation_artifact: ModelEvaluationArtifact = evaluator.run_evaluation()

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Model Pusher
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            pusher_config = self.config_manager.get_model_pusher_config()
            pusher = ModelPusher(model_pusher_config=pusher_config, model_trainer_artifact=trainer_artifact)
            pusher_artifact: ModelPusherArtifact = pusher.push_model()

            logger.info("========== Training Pipeline Completed Successfully ==========")
            return pusher_artifact

        except Exception as e:
            logger.exception("Training Pipeline Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\utils\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\utils\core.py
================================================================================

import json
import joblib
import pandas as pd
import numpy as np
import yaml
from pathlib import Path
from box import ConfigBox
from box.exceptions import BoxKeyError, BoxTypeError, BoxValueError
from urllib.parse import quote_plus
from ensure import ensure_annotations

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


@ensure_annotations
def read_yaml(path_to_yaml: Path) -> ConfigBox:
    """
    Load a YAML file as a ConfigBox, always using UTF-8.
    """
    if not path_to_yaml.exists():
        msg = f"YAML file not found: '{path_to_yaml}'"
        logger.error(msg)
        raise NetworkSecurityError(FileNotFoundError(msg), logger)

    try:
        # Explicitly open as UTF-8 to avoid Windows cp1252 issues
        with path_to_yaml.open("r", encoding="utf-8") as f:
            content = yaml.safe_load(f)
    except (BoxValueError, BoxTypeError, BoxKeyError, yaml.YAMLError) as e:
        logger.error(f"Failed to load YAML from {path_to_yaml.as_posix()}: {e}")
        raise NetworkSecurityError(e, logger) from e
    except Exception as e:
        logger.error(f"Unexpected error reading YAML file: {e}")
        raise NetworkSecurityError(e, logger) from e

    if content is None:
        msg = "YAML file is empty or improperly formatted."
        logger.error(msg)
        raise NetworkSecurityError(ValueError(msg), logger)

    logger.info(f"YAML loaded successfully from: '{path_to_yaml.as_posix()}'")
    return ConfigBox(content)


@ensure_annotations
def csv_to_json_convertor(source_filepath: Path, destination_filepath: Path):
    """
    Convert a CSV file to a list of JSON records and optionally save it.
    """
    try:
        if not source_filepath.exists():
            raise FileNotFoundError(f"CSV source file not found: '{source_filepath.as_posix()}'")

        df = pd.read_csv(source_filepath).reset_index(drop=True)
        records = df.to_dict(orient="records")

        parent_dir = destination_filepath.parent
        if not parent_dir.exists():
            parent_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created directory for JSON output: '{parent_dir.as_posix()}'")
        else:
            logger.info(f"Directory already exists for JSON output: '{parent_dir.as_posix()}'")

        with destination_filepath.open("w", encoding="utf-8") as f:
            json.dump(records, f, indent=4)

        logger.info(f"CSV converted and saved to: '{destination_filepath.as_posix()}'")
        return records

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def replace_username_password_in_uri(base_uri: str, username: str, password: str) -> str:
    if not all([base_uri, username, password]):
        raise ValueError("base_uri, username, and password must all be provided.")

    encoded_username = quote_plus(username)
    encoded_password = quote_plus(password)

    return (
        base_uri
        .replace("<username>", encoded_username)
        .replace("<password>", encoded_password)
    )


@ensure_annotations
def save_to_yaml(data: dict, *paths: Path, label: str):
    """
    Write a dict out to YAML, always using UTF-8.
    """
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            # Write UTF-8
            with open(path, "w", encoding="utf-8") as file:
                yaml.dump(data, file, sort_keys=False)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def save_to_csv(df: pd.DataFrame, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            df.to_csv(path, index=False)
            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def save_array(array: np.ndarray | pd.Series, *paths: Path, label: str):
    """
    Saves a NumPy array or pandas Series to the specified paths in `.npy` format.

    Args:
        array (Union[np.ndarray, pd.Series]): Data to save.
        *paths (Path): One or more file paths.
        label (str): Label for logging.
    """
    try:
        # Convert Series to ndarray if needed
        array = np.asarray(array)

        for path in paths:
            path = Path(path)

            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            # # Ensure file ends with `.npy`
            # if path.suffix != ".npy":
            #     path = path.with_suffix(".npy")

            np.save(path, array)
            logger.info(f"{label} saved to: '{path.as_posix()}'")

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e
    
@ensure_annotations
def load_array(path: Path, label: str) -> np.ndarray:
    """
    Loads a NumPy array from the specified `.npy` file path.

    Args:
        path (Path): Path to the `.npy` file.
        label (str): Label for logging.

    Returns:
        np.ndarray: Loaded NumPy array.
    """
    try:
        path = Path(path)

        if not path.exists():
            raise FileNotFoundError(f"{label} file not found at path: '{path.as_posix()}'")

        array = np.load(path)
        logger.info(f"{label} loaded successfully from: '{path.as_posix()}'")
        return array

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def save_to_json(data: dict, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            with open(path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=4)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def read_csv(path: Path, label: str) -> pd.DataFrame:
    try:
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"{label} not found at path: {path.as_posix()}")

        df = pd.read_csv(path)
        logger.info(f"{label} loaded from: '{path.as_posix()}'")
        return df

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def save_object(obj: object, path: Path, label: str):
    try:
        path = Path(path)
        if not path.parent.exists():
            path.parent.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
        else:
            logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

        joblib.dump(obj, path)
        logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def load_object(path: Path):
    """
    Load a serialized Python object from the given path using joblib.

    Args:
        path (Path): Path to the saved object file.

    Returns:
        object: The deserialized Python object.

    Raises:
        NetworkSecurityError: If loading fails.
    """
    try:
        path = Path(path)
        logger.info(f"Loading object from: '{path.as_posix()}'")
        return joblib.load(path)
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\utils\timestamp.py
================================================================================

from datetime import datetime, timezone

# Internal module-level cache to ensure the timestamp is consistent across the entire pipeline
_timestamp_cache: str = None

def get_shared_utc_timestamp(fmt: str = "%Y_%m_%dT%H_%M_%SZ") -> str:
    """
    Returns a consistent, cached UTC timestamp for the current pipeline run.

    This function guarantees that:
    - The timestamp is generated only once.
    - All modules (logger, config, etc.) use the exact same value.
    - The format is clean and safe for filenames, directories, and versioning.

    Args:
        fmt (str): Optional datetime format string.
                   Default: 'YYYY_MM_DDTHH_MM_SSZ' (e.g., '2025_04_16T17_45_02Z')

    Returns:
        str: UTC timestamp string formatted per the provided format.
    """
    global _timestamp_cache
    if _timestamp_cache is None:
        _timestamp_cache = datetime.now(timezone.utc).strftime(fmt)
    return _timestamp_cache

================================================================================
# PY FILE: src\networksecurity\worker\celery_worker.py
================================================================================

from celery import Celery
from src.networksecurity.pipeline.training_pipeline import TrainingPipeline

celery_app = Celery(
    "networksecurity_tasks",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/0",
)

@celery_app.task(name="trigger_training_task")
def trigger_training_task():
    pipeline = TrainingPipeline()
    pipeline.run_pipeline()

================================================================================
# YAML FILE: config\config.yaml
================================================================================

# Root directory for all experiment artifacts
project:
  artifacts_root: artifacts

# MongoDB configuration
mongo_handler:
  input_data_path: network_data/input_csv/phisingData.csv
  json_data_filename: input_data.json
  database_name: network_security_db
  collection_name: phishing_records

# Data ingestion configuration
data_ingestion:
  raw_data_filename: raw_data.csv
  ingested_data_filename: ingested_data.csv

# Data validation configuration
data_validation:
  validated_filename: validated_data.csv
  missing_report_filename: missing_values_report.json
  duplicates_report_filename: duplicates_report.yaml
  drift_report_filename: drift_report.yaml
  validation_report_filename: validation_report.yaml
  schema_hash_filename: schema_hash.json


# Data transformation configuration
data_transformation:
  x_train_filename: x_train.npy
  y_train_filename: y_train.npy
  x_val_filename: x_val.npy
  y_val_filename: y_val.npy
  x_test_filename: x_test.npy
  y_test_filename: y_test.npy
  x_preprocessor_filename: x_preprocessor.joblib
  y_preprocessor_filename: y_preprocessor.joblib
  

# Model trainer configuration
model_trainer:
  model_dir: saved_models
  trained_model_filename: model.joblib
  # where to dump a short training report
  training_report_filename: training_report.yaml

# Model prediction configuration
model_prediction:
  prediction_output_filename: prediction_output.csv

# Stable DVC-tracked data paths
data_paths:
  raw_data_dvc_filepath: data/raw/raw_data.csv
  validated_dvc_filepath: data/validated/validated_data.csv
  train_dvc_dir: data/transformed/train
  val_dvc_dir: data/transformed/val
  test_dvc_dir: data/transformed/test

# Model evaluation configuration
model_evaluation:
  evaluation_report_filename: evaluation_report.yaml
  evaluated_model_filename: evaluated_model.joblib

model_pusher:
  final_model_filename: final_inference_model.joblib
  final_model_s3_bucket: networksecurity-dev-artifacts
  s3_final_model_folder: final_model
  s3_artifacts_folder: artifacts
  upload_to_s3: true
  aws_region: us-east-1

================================================================================
# YAML FILE: config\params.yaml
================================================================================

# Parameters for drift detection during data validation
validation_params:
  drift_detection:
    enabled: true
    method: ks_test
    p_value_threshold: 0.05

  schema_check:
    enabled: true
    method: hash


# Parameters for data transformation
transformation_params:
  # Parameters for data splitting
  data_split:
    train_size: 0.6
    test_size: 0.2
    val_size: 0.2
    random_state: 42
    stratify: true

  steps:
    x:
      imputer: knn
    y:
      label_mapping: label_mapper

  methods:
    x:
      imputer:
        missing_values: null
        n_neighbors: 3
        weights: uniform
    y:
      label_mapping:
        from: -1
        to: 0




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parameters for model training
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parameters for model training
model_trainer:


  # candidate estimators (importable by sklearn convention)
  models:
    - name: sklearn.ensemble.RandomForestClassifier
      # default params (used if optimization.enabled = false)
      params:
        n_estimators: 100
        max_depth: 10
        random_state: 42
      # search-space for Optuna
      search_space:
        n_estimators:
          distribution: int
          low: 50
          high: 300
          step: 10
        max_depth:
          distribution: int
          low: 5
          high: 50
          step: 1

    - name: sklearn.ensemble.GradientBoostingClassifier
      params:
        n_estimators: 100
        learning_rate: 0.1
        max_depth: 3
        random_state: 42
      search_space:
        n_estimators:
          distribution: int
          low: 50
          high: 200
          step: 10
        learning_rate:
          distribution: float
          low: 0.01
          high: 1.0
          log: true
        max_depth:
          distribution: int
          low: 2
          high: 10

  # hyperparameter optimization settings
  optimization:
    enabled: true
    method: optuna
    n_trials: 30
    direction: maximize
    cv_folds: 5
    scoring: accuracy

  # MLflow tracking & registry settings (URI is now picked from ENV)
  tracking:
    mlflow:
      enabled: true
      experiment_name: NetworkSecurityExperiment
      registry_model_name: NetworkSecurityModel
      metrics_to_log:
        - accuracy
        - f1
        - precision
        - recall
      # new switch: whether to log each Optuna trial as its own MLflow run
      log_trials: false

================================================================================
# YAML FILE: config\schema.yaml
================================================================================

columns:
  Abnormal_URL: int64
  DNSRecord: int64
  Domain_registeration_length: int64
  Favicon: int64
  Google_Index: int64
  HTTPS_token: int64
  Iframe: int64
  Links_in_tags: int64
  Links_pointing_to_page: int64
  Page_Rank: int64
  Prefix_Suffix: int64
  Redirect: int64
  Request_URL: int64
  Result: int64
  RightClick: int64
  SFH: int64
  SSLfinal_State: int64
  Shortining_Service: int64
  Statistical_report: int64
  Submitting_to_email: int64
  URL_Length: int64
  URL_of_Anchor: int64
  age_of_domain: int64
  double_slash_redirecting: int64
  having_At_Symbol: int64
  having_IP_Address: int64
  having_Sub_Domain: int64
  on_mouseover: int64
  popUpWidnow: int64
  port: int64
  web_traffic: int64

target_column: Result

target_labels:
  0: phishing
  1: legitimate

================================================================================
# YAML FILE: research\config\schema.yaml
================================================================================

columns:
  Abnormal_URL: int64
  DNSRecord: int64
  Domain_registeration_length: int64
  Favicon: int64
  Google_Index: int64
  HTTPS_token: int64
  Iframe: int64
  Links_in_tags: int64
  Links_pointing_to_page: int64
  Page_Rank: int64
  Prefix_Suffix: int64
  Redirect: int64
  Request_URL: int64
  Result: int64
  RightClick: int64
  SFH: int64
  SSLfinal_State: int64
  Shortining_Service: int64
  Statistical_report: int64
  Submitting_to_email: int64
  URL_Length: int64
  URL_of_Anchor: int64
  age_of_domain: int64
  double_slash_redirecting: int64
  having_At_Symbol: int64
  having_IP_Address: int64
  having_Sub_Domain: int64
  on_mouseover: int64
  popUpWidnow: int64
  port: int64
  web_traffic: int64
