
ðŸ“¦ Project Structure of: E:\MyProjects\networksecurity

ðŸ“ .dvc/
    ðŸ“„ .gitignore
    ðŸ“ cache/
        ðŸ“ files/
            ðŸ“ md5/
                ðŸ“ 68/
                    ðŸ“„ 770e0efceb9cd3777502f505ecbec4
                ðŸ“ 70/
                    ðŸ“„ 3506878baa6bf0360b2b42782e9677
                ðŸ“ 83/
                    ðŸ“„ de4e3a0dbe7d93403a8c15753377f7
                ðŸ“ 86/
                    ðŸ“„ 4e5c8474fcd1b15bc434932036628d
                ðŸ“ 87/
                    ðŸ“„ f220430be73d16609bfd637b1644af
                ðŸ“ 8d/
                    ðŸ“„ e71e17a20f71bece7659a34bae7027
                ðŸ“ bf/
                    ðŸ“„ fa4350e8aa16caaa3cbb92ec8e048c
                ðŸ“ e1/
                    ðŸ“„ a8ae356f072436dd3acc1bcbafbd89
    ðŸ“„ config
    ðŸ“ tmp/
        ðŸ“„ btime
        ðŸ“„ dag.md
        ðŸ“ exps/
            ðŸ“ cache/
                ðŸ“ 08/
                    ðŸ“„ ba3af380fc7743bb7784b0b99b9e84b597659c
                ðŸ“ 0c/
                    ðŸ“„ 7b66bd77f8e71f84c28f8868f79d1c391b243e
                ðŸ“ 17/
                    ðŸ“„ a15f92afbc3a237675719f1bb52d4e45b2855f
                ðŸ“ 20/
                    ðŸ“„ 418b499ec78701cefc3c979011ef77fa37084d
                ðŸ“ 28/
                    ðŸ“„ 1710634e34c59aaca7c823b82ba97e8c3ae6c8
                    ðŸ“„ e888b2e08a4b7dc34604b66abd2af2aaed6b86
                ðŸ“ 29/
                    ðŸ“„ 28b4f351c2d8f210468c693d53455c7b09a1eb
                    ðŸ“„ dfeb7151d1e0fc143507ecdecb1bedd2b5cc4d
                ðŸ“ 2e/
                    ðŸ“„ d0f21b70c06ff15b188bb69f6ce389a869af78
                ðŸ“ 32/
                    ðŸ“„ 9b746e5971f793e269ced861c2d1e401f43d48
                ðŸ“ 3e/
                    ðŸ“„ a84c7b28b0242174e45334d5d6a9d3026f546d
                ðŸ“ 48/
                    ðŸ“„ faf510268751ce441d9178a563f5e2c4577d88
                ðŸ“ 4a/
                    ðŸ“„ 640ce33b51ffeb8c1aa85e520ce78dfe105123
                ðŸ“ 4c/
                    ðŸ“„ 0bc699b6997f940ba6b39f9c3f002d74b0cd52
                ðŸ“ 51/
                    ðŸ“„ a289cbadb16786fe46c2b4bab48853efc637a9
                ðŸ“ 52/
                    ðŸ“„ 0a8ade47b7e748a46de7cd94c1f1db7a925527
                    ðŸ“„ b49095c9d6e6a69ba58f77d1979c4626e30907
                ðŸ“ 61/
                    ðŸ“„ a81d71c590a7a662176a82aa5f00a7532a3e16
                ðŸ“ 63/
                    ðŸ“„ 02ec043a14cb6881a1196ca1b58f7df093a586
                    ðŸ“„ b517b2eff95eb91aba4bfef87abcd7955ae01c
                ðŸ“ 64/
                    ðŸ“„ 3172fd8ac68c1fe35ae5f9a8b4734633a30778
                ðŸ“ 67/
                    ðŸ“„ fee62ed0bb527f9980ea136f0c312784242b38
                ðŸ“ 68/
                    ðŸ“„ 81874523aeb9b0b77eb7bae250d0287e341558
                ðŸ“ 6c/
                    ðŸ“„ 4d54822a8eec14b0c0ea4846ba14c9541e31dd
                ðŸ“ 7f/
                    ðŸ“„ e668c86a64d0171bc4dc8708460ac48ec77a34
                ðŸ“ 88/
                    ðŸ“„ a8f98863afa828023b2e3616348335b6d4628e
                ðŸ“ 89/
                    ðŸ“„ daed7059804441e9041b6d073c071a1cfcbe2c
                ðŸ“ 8a/
                    ðŸ“„ a79a390fba4858401f3e79d94bee5dbf354341
                    ðŸ“„ ed08439cb37bbee9fee4ad00d82ea30435606b
                ðŸ“ a5/
                    ðŸ“„ 168ff7e9460e593e744f60ef1bad04413e1ca7
                ðŸ“ a8/
                    ðŸ“„ 2c1cb5bbc048ca32cb78061a4b8aaf3735233f
                ðŸ“ ad/
                    ðŸ“„ 329b2130c11421d465202bdf74f5170e661c23
                ðŸ“ af/
                    ðŸ“„ ea20d43951b4448cd0a4eb4a7a134958cfb5c5
                ðŸ“ b0/
                    ðŸ“„ 63569211a95f11f23c4f61f6b9518bbf3b2cf3
                ðŸ“ b2/
                    ðŸ“„ a55eaf06589f37339db03046e528a4eb70f795
                ðŸ“ bb/
                    ðŸ“„ 6504b5b79c287b4b60d2355251d8b30025c769
                    ðŸ“„ e1aa1f0748e2a41f3ece907bfaaae1cafb5c48
                ðŸ“ c4/
                    ðŸ“„ 9033287ca54d77ad6ab9c7603c06825bbac4e6
                ðŸ“ c7/
                    ðŸ“„ 31a093cb23b7a2bef65922d39d9d20ac56e6f3
                ðŸ“ c8/
                    ðŸ“„ dbdb66a143e984e4fe8b43c7d755441cdec3f5
                ðŸ“ ce/
                    ðŸ“„ 1c26eef879f92adf044d9d0da37c028fd7d494
                    ðŸ“„ 4e08484f42af33f1129dda5084bf3fed271a22
                ðŸ“ d2/
                    ðŸ“„ 3e801f5cac5d3a86a0e23a6fdbe1cbe281f789
                ðŸ“ d6/
                    ðŸ“„ 4f71f9b0de7fb7fca7d5f21249de0a489b4ca8
                ðŸ“ dc/
                    ðŸ“„ fff6f4ba5cf77e9a0eaf75c0830d9795bb58cf
                ðŸ“ dd/
                    ðŸ“„ ee659d7114460f19e1804a98117314724c9b04
                ðŸ“ e3/
                    ðŸ“„ 5de325a2125360b5ae289c080fc748a0140cdc
                    ðŸ“„ 8f209c48f47e7a864c8c9c61048ce16f1a2896
                ðŸ“ eb/
                    ðŸ“„ 79e8cfe8c4a974d876650c1a48de56761ef2a1
                ðŸ“ f3/
                    ðŸ“„ 8d3509705265cc0399a9e13b577074bfb9aedf
                ðŸ“ f6/
                    ðŸ“„ f9d61771a2404a0d798efa45ff7ff15c8f639a
                ðŸ“ f9/
                    ðŸ“„ 21cb7aa3662a13f16a67e07a71781cec9b1a91
            ðŸ“ celery/
                ðŸ“ broker/
                    ðŸ“ control/
                    ðŸ“ in/
                    ðŸ“ processed/
                ðŸ“ result/
        ðŸ“„ lock
        ðŸ“„ rwlock
        ðŸ“„ rwlock.lock
ðŸ“„ .dvcignore
ðŸ“„ .env
ðŸ“„ .gitignore
ðŸ“„ Dockerfile
ðŸ“„ LICENSE
ðŸ“ NetworkSecurity.egg-info/
    ðŸ“„ PKG-INFO
    ðŸ“„ SOURCES.txt
    ðŸ“„ dependency_links.txt
    ðŸ“„ requires.txt
    ðŸ“„ top_level.txt
ðŸ“„ README.md
ðŸ“„ app.py
ðŸ“ artifacts/
    ðŸ“ 2025_05_24T09_51_21Z/
        ðŸ“ data_ingestion/
            ðŸ“ featurestore/
                ðŸ“„ raw_data.csv
            ðŸ“ ingested/
                ðŸ“„ ingested_data.csv
        ðŸ“ data_transformation/
            ðŸ“ preprocessor/
                ðŸ“„ x_preprocessor.joblib
                ðŸ“„ y_preprocessor.joblib
            ðŸ“ transformed/
                ðŸ“ test/
                    ðŸ“„ x_test.npy
                    ðŸ“„ y_test.npy
                ðŸ“ train/
                    ðŸ“„ x_train.npy
                    ðŸ“„ y_train.npy
                ðŸ“ val/
                    ðŸ“„ x_val.npy
                    ðŸ“„ y_val.npy
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.json
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
        ðŸ“ model_evaluation/
            ðŸ“„ evaluation_report.yaml
        ðŸ“ model_trainer/
            ðŸ“ inference_model/
                ðŸ“„ inference_model.joblib
            ðŸ“ reports/
                ðŸ“„ training_report.yaml
            ðŸ“ trained_model/
                ðŸ“„ model.joblib
ðŸ“ config/
    ðŸ“„ config.yaml
    ðŸ“„ params.yaml
    ðŸ“„ schema.yaml
    ðŸ“„ templates.yaml
ðŸ“ data/
    ðŸ“ raw/
        ðŸ“„ raw_data.csv
        ðŸ“„ raw_data.csv.dvc
    ðŸ“ transformed/
        ðŸ“ test/
            ðŸ“„ x_test.npy
            ðŸ“„ x_test.npy.dvc
            ðŸ“„ y_test.npy
            ðŸ“„ y_test.npy.dvc
        ðŸ“ train/
            ðŸ“„ x_train.npy
            ðŸ“„ x_train.npy.dvc
            ðŸ“„ y_train.npy
            ðŸ“„ y_train.npy.dvc
        ðŸ“ val/
            ðŸ“„ x_val.npy
            ðŸ“„ x_val.npy.dvc
            ðŸ“„ y_val.npy
            ðŸ“„ y_val.npy.dvc
    ðŸ“ validated/
        ðŸ“„ validated_data.csv
        ðŸ“„ validated_data.csv.dvc
ðŸ“„ debug.py
ðŸ“„ debug_pipeline.py
ðŸ“„ docker-compose.yaml
ðŸ“ final_model/
    ðŸ“„ final_inference_model.joblib
ðŸ“ logs/
    ðŸ“ 2025_05_24T09_51_21Z/
        ðŸ“„ 2025_05_24T09_51_21Z.log
ðŸ“„ main.py
ðŸ“ network_data/
    ðŸ“ input_csv/
        ðŸ“„ phisingData.csv
        ðŸ“„ phisingData_check.csv
ðŸ“ prediction_output/
ðŸ“„ print_structure.py
ðŸ“„ project_dump.py
ðŸ“„ project_template.py
ðŸ“„ requirements.txt
ðŸ“ research/
    ðŸ“ config/
        ðŸ“„ schema.yaml
    ðŸ“„ ingested_data.csv
    ðŸ“„ research.ipynb
ðŸ“„ setup.py
ðŸ“ src/
    ðŸ“ networksecurity/
        ðŸ“„ __init__.py
        ðŸ“ components/
            ðŸ“„ __init__.py
            ðŸ“„ data_ingestion.py
            ðŸ“„ data_transformation.py
            ðŸ“„ data_validation.py
            ðŸ“„ model_evaluation.py
            ðŸ“„ model_pusher.py
            ðŸ“„ model_trainer.py
        ðŸ“ config/
            ðŸ“„ __init__.py
            ðŸ“„ configuration.py
        ðŸ“ constants/
            ðŸ“„ __init__.py
            ðŸ“„ constants.py
        ðŸ“ data_processors/
            ðŸ“„ encoder_factory.py
            ðŸ“„ imputer_factory.py
            ðŸ“„ label_mapper.py
            ðŸ“„ preprocessor_builder.py
            ðŸ“„ scaler_factory.py
        ðŸ“ dbhandler/
            ðŸ“„ __init__.py
            ðŸ“„ base_handler.py
            ðŸ“„ mongodb_handler.py
            ðŸ“„ s3_handler.py
        ðŸ“ entity/
            ðŸ“„ __init__.py
            ðŸ“„ artifact_entity.py
            ðŸ“„ config_entity.py
        ðŸ“ exception/
            ðŸ“„ __init__.py
            ðŸ“„ exception.py
        ðŸ“ inference/
            ðŸ“„ estimator.py
        ðŸ“ logging/
            ðŸ“„ __init__.py
            ðŸ“„ logger.py
        ðŸ“ pipeline/
            ðŸ“„ __init__.py
            ðŸ“„ data_ingestion_pipeline.py
            ðŸ“„ data_transformation_pipeline.py
            ðŸ“„ data_validation_pipeline.py
            ðŸ“„ model_evaluation_pipeline.py
            ðŸ“„ model_pusher_pipeline.py
            ðŸ“„ model_trainer_pipeline.py
            ðŸ“„ training_pipeline.py
        ðŸ“ utils/
            ðŸ“„ __init__.py
            ðŸ“„ core.py
            ðŸ“„ timestamp.py
        ðŸ“ worker/
            ðŸ“„ celery_worker.py
ðŸ“ templates/
    ðŸ“„ table.html

--- CODE DUMP | PART 3 of 4 ---


================================================================================
# PY FILE: src\networksecurity\dbhandler\s3_handler.py
================================================================================

from pathlib import Path
import os
import boto3
from botocore.exceptions import ClientError

import pandas as pd

from src.networksecurity.entity.config_entity import S3HandlerConfig
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from src.networksecurity.dbhandler.base_handler import DBHandler


class S3Handler(DBHandler):
    """
    AWS S3 Handler implementing DBHandler interface.
    Supports file upload and directory sync.
    """

    def __init__(self, config: S3HandlerConfig):
        try:
            self.config = config
            self._client = boto3.client("s3", region_name=self.config.aws_region)
            logger.info(f"S3Handler initialized for bucket '{self.config.bucket_name}' in region '{self.config.aws_region}'")
        except Exception as e:
            logger.exception("Failed to initialize S3 client")
            raise NetworkSecurityError(e, logger) from e

    def close(self) -> None:
        """
        No persistent connection in S3 client.
        Included for interface compatibility.
        """
        logger.info("S3Handler close() called. No persistent connection to close.")

    def load_from_source(self) -> pd.DataFrame:
        """
        Not supported: Loading data as DataFrame from S3 is not implemented.
        """
        raise NotImplementedError("S3Handler does not support DataFrame loading from S3.")

    def upload_file(self, local_path: Path, s3_key: str) -> None:
        """
        Upload a single file to S3.

        Args:
            local_path (Path): Local file path.
            s3_key (str): Destination S3 key (folder/filename).
        """
        try:
            local_path = Path(local_path)
            if not local_path.is_file():
                raise FileNotFoundError(f"Local file not found: {local_path.as_posix()}")

            self._client.upload_file(
                Filename=str(local_path),
                Bucket=self.config.bucket_name,
                Key=s3_key
            )
            logger.info(f"Uploaded: {local_path.as_posix()} -> s3://{self.config.bucket_name}/{s3_key}")

        except ClientError as e:
            logger.error(f"AWS ClientError while uploading to S3: {e}")
            raise NetworkSecurityError(e, logger) from e
        except Exception as e:
            logger.error(f"Unexpected error uploading to S3: {e}")
            raise NetworkSecurityError(e, logger) from e

    def sync_directory(self, local_dir: Path, s3_prefix: str) -> None:
        """
        Uploads a directory to S3 recursively.

        Args:
            local_dir (Path): Local directory to sync.
            s3_prefix (str): S3 prefix (target folder path).
        """
        try:
            local_dir = Path(local_dir)
            if not local_dir.is_dir():
                raise NotADirectoryError(f"Local directory not found: {local_dir.as_posix()}")

            logger.info(f"Starting S3 sync: {local_dir.as_posix()} -> s3://{self.config.bucket_name}/{s3_prefix}")

            for root, _, files in os.walk(local_dir):
                for file in files:
                    local_file_path = Path(root) / file
                    relative_path = local_file_path.relative_to(local_dir)
                    remote_key = (Path(s3_prefix) / relative_path).as_posix()
                    self.upload_file(local_file_path, remote_key)

            logger.info(f"Directory synced to S3: {local_dir.as_posix()} -> s3://{self.config.bucket_name}/{s3_prefix}")

        except Exception as e:
            logger.error("Directory sync to S3 failed.")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\entity\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\entity\artifact_entity.py
================================================================================

from dataclasses import dataclass
from pathlib import Path
from typing import Optional


@dataclass(frozen=True)
class DataIngestionArtifact:
    raw_artifact_path: Path
    ingested_data_filepath: Path
    raw_dvc_path: Path

    def __repr__(self) -> str:
        raw_artifact_str = self.raw_artifact_path.as_posix() if self.raw_artifact_path else "None"
        raw_dvc_str = self.raw_dvc_path.as_posix() if self.raw_dvc_path else "None"
        ingested_data_str = self.ingested_data_filepath.as_posix() if self.ingested_data_filepath else "None"

        return (
            "\nData Ingestion Artifact:\n"
            f"  - Raw Artifact:         '{raw_artifact_str}'\n"
            f"  - Raw DVC Path:         '{raw_dvc_str}'\n"
            f"  - Ingested Data Path:   '{ingested_data_str}'\n"
        )


@dataclass(frozen=True)
class DataValidationArtifact:
    validated_filepath: Path
    validation_status: bool

    def __repr__(self) -> str:
        validated_str = self.validated_filepath.as_posix() if self.validated_filepath else "None"

        return (
            "\nData Validation Artifact:\n"
            f"  - Validated Data Path: '{validated_str}'\n"
            f"  - Validation Status:   '{self.validation_status}'\n"
        )

@dataclass(frozen=True)
class DataTransformationArtifact:
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path
    x_preprocessor_filepath: Path
    y_preprocessor_filepath: Path

    def __repr__(self) -> str:
        x_train_str = self.x_train_filepath.as_posix() if self.x_train_filepath else "None"
        y_train_str = self.y_train_filepath.as_posix() if self.y_train_filepath else "None"
        x_val_str = self.x_val_filepath.as_posix() if self.x_val_filepath else "None"
        y_val_str = self.y_val_filepath.as_posix() if self.y_val_filepath else "None"
        x_test_str = self.x_test_filepath.as_posix() if self.x_test_filepath else "None"
        y_test_str = self.y_test_filepath.as_posix() if self.y_test_filepath else "None"
        x_preprocessor_str = self.x_preprocessor_filepath.as_posix() if self.x_preprocessor_filepath else "None"
        y_preprocessor_str = self.y_preprocessor_filepath.as_posix() if self.y_preprocessor_filepath else "None"

        return (
            "\nData Transformation Artifact:\n"
            f"  - X-Train Data Path:    '{x_train_str}'\n"
            f"  - Y-Train Data Path:    '{y_train_str}'\n"
            f"  - X-Val Data Path:      '{x_val_str}'\n"
            f"  - Y-Val Data Path:      '{y_val_str}'\n"
            f"  - X-Test Data Path:     '{x_test_str}'\n"
            f"  - Y-Test Data Path:     '{y_test_str}'\n"
            f"  - X-Processor Path:     '{x_preprocessor_str}'\n"
            f"  - Y-Processor Path:     '{y_preprocessor_str}'\n"
        )


@dataclass(frozen=True)
class ModelTrainerArtifact:
    trained_model_filepath: Path
    training_report_filepath: Path
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path

    def __repr__(self) -> str:
        return (
            "\nModel Trainer Artifact:\n"
            f"  - Trained Model Path:   '{self.trained_model_filepath.as_posix()}'\n"
            f"  - Training Report Path: '{self.training_report_filepath.as_posix()}'\n"
            f"  - X Train Path: '{self.x_train_filepath.as_posix()}'\n"
            f"  - Y Train Path: '{self.y_train_filepath.as_posix()}'\n"
            f"  - X Val Path:   '{self.x_val_filepath.as_posix()}'\n"
            f"  - Y Val Path:   '{self.y_val_filepath.as_posix()}'\n"
            f"  - X Test Path:  '{self.x_test_filepath.as_posix()}'\n"
            f"  - Y Test Path:  '{self.y_test_filepath.as_posix()}'"
        )


@dataclass(frozen=True)
class ModelEvaluationArtifact:
    evaluation_report_filepath: Path

    def __repr__(self) -> str:
        report_str = self.evaluation_report_filepath.as_posix() if self.evaluation_report_filepath else "None"

        return (
            "\nModel Evaluation Artifact:\n"
            f"  - Evaluation Report Path: '{report_str}'\n"
        )

@dataclass(frozen=True)
class ModelPusherArtifact:
    pushed_model_local_path: Path
    pushed_model_s3_path: str

    def __repr__(self) -> str:
        local_str = self.pushed_model_local_path.as_posix() if self.pushed_model_local_path else "None"
        s3_str = self.pushed_model_s3_path if self.pushed_model_s3_path else "None"
        return (
            "\nModel Pusher Artifact:\n"
            f"  - Local Path: '{local_str}'\n"
            f"  - S3 Path:    '{s3_str}'\n"
        )


@dataclass(frozen=True)
class ModelPusherArtifact:
    pushed_model_local_path: Path
    pushed_model_s3_path: str | None = None  # Optional if S3 upload is disabled

    def __repr__(self) -> str:
        return (
            "\nModel Pusher Artifact:\n"
            f"  - Local Model Path: {self.pushed_model_local_path.as_posix()}\n"
            f"  - S3 Model Path:    {self.pushed_model_s3_path or 'Not uploaded'}\n"
        )

================================================================================
# PY FILE: src\networksecurity\entity\config_entity.py
================================================================================

from pathlib import Path
from dataclasses import dataclass
from box import ConfigBox
from typing import List


@dataclass
class MongoHandlerConfig:
    root_dir: Path
    input_data_path: Path
    json_data_filename: str
    json_data_dir: Path
    mongodb_uri: str
    database_name: str
    collection_name: str

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.input_data_path = Path(self.input_data_path)
        self.json_data_dir = Path(self.json_data_dir)

    @property
    def json_data_filepath(self) -> Path:
        return self.json_data_dir / self.json_data_filename


@dataclass
class DataIngestionConfig:
    root_dir: Path
    featurestore_dir: Path
    raw_data_filename: str
    ingested_data_dir: Path
    ingested_data_filename: str
    raw_dvc_path: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.featurestore_dir = Path(self.featurestore_dir)
        self.ingested_data_dir = Path(self.ingested_data_dir)
        self.raw_dvc_path = Path(self.raw_dvc_path)

    @property
    def raw_data_filepath(self) -> Path:
        return self.featurestore_dir / self.raw_data_filename

    @property
    def ingested_data_filepath(self) -> Path:
        return self.ingested_data_dir / self.ingested_data_filename


@dataclass
class DataValidationConfig:
    root_dir: Path
    validated_dir: Path
    validated_filename: str
    report_dir: Path
    missing_report_filename: str
    duplicates_report_filename: str
    drift_report_filename: str
    validation_report_filename: str
    schema: dict
    validation_params: dict
    validated_dvc_path: Path
    val_report_template: dict

    def __post_init__(self) -> Path:
        self.root_dir = Path(self.root_dir)
        self.validated_dir = Path(self.validated_dir)
        self.report_dir = Path(self.report_dir)
        self.validated_dvc_path = Path(self.validated_dvc_path)

    @property
    def validated_filepath(self) -> Path:
        return self.validated_dir / self.validated_filename

    @property
    def missing_report_filepath(self) -> Path:
        return self.report_dir / self.missing_report_filename

    @property
    def duplicates_report_filepath(self) -> Path:
        return self.report_dir / self.duplicates_report_filename

    @property
    def drift_report_filepath(self) -> Path:
        return self.report_dir / self.drift_report_filename

    @property
    def validation_report_filepath(self) -> Path:
        return self.report_dir / self.validation_report_filename


@dataclass
class DataTransformationConfig:
    root_dir: Path
    transformation_params: dict
    train_dir: Path
    val_dir: Path
    test_dir: Path
    x_train_filename: str
    y_train_filename: str
    x_val_filename: str
    y_val_filename: str
    x_test_filename: str
    y_test_filename: str
    preprocessor_dir: Path
    x_preprocessor_filename: str
    y_preprocessor_filename: str
    target_column: str
    train_dvc_dir: Path
    val_dvc_dir: Path
    test_dvc_dir: Path

    def __post_init__(self) -> Path:
        self.root_dir = Path(self.root_dir)
        self.train_dir = Path(self.train_dir)
        self.val_dir = Path(self.val_dir)
        self.test_dir = Path(self.test_dir)
        self.preprocessor_dir = Path(self.preprocessor_dir)

    @property
    def x_train_filepath(self) -> Path:
        return self.train_dir / self.x_train_filename

    @property
    def y_train_filepath(self) -> Path:
        return self.train_dir / self.y_train_filename

    @property
    def x_val_filepath(self) -> Path:
        return self.val_dir / self.x_val_filename

    @property
    def y_val_filepath(self) -> Path:
        return self.val_dir / self.y_val_filename

    @property
    def x_test_filepath(self) -> Path:
        return self.test_dir / self.x_test_filename

    @property
    def y_test_filepath(self) -> Path:
        return self.test_dir / self.y_test_filename

    @property
    def x_preprocessor_filepath(self) -> Path:
        return self.preprocessor_dir / self.x_preprocessor_filename

    @property
    def y_preprocessor_filepath(self) -> Path:
        return self.preprocessor_dir / self.y_preprocessor_filename

    @property
    def x_train_dvc_filepath(self) -> Path:
        return self.train_dvc_dir / self.x_train_filename

    @property
    def y_train_dvc_filepath(self) -> Path:
        return self.train_dvc_dir / self.y_train_filename

    @property
    def x_val_dvc_filepath(self) -> Path:
        return self.val_dvc_dir / self.x_val_filename

    @property
    def y_val_dvc_filepath(self) -> Path:
        return self.val_dvc_dir / self.y_val_filename

    @property
    def x_test_dvc_filepath(self) -> Path:
        return self.test_dvc_dir / self.x_test_filename

    @property
    def y_test_dvc_filepath(self) -> Path:
        return self.test_dvc_dir / self.y_test_filename



@dataclass
class ModelTrainerConfig:
    root_dir: Path
    trained_model_filename: str
    training_report_filename: str
    models: list[dict]
    optimization: dict
    tracking: dict
    models: List[dict]
    optimization: ConfigBox
    tracking: ConfigBox
    # where to load transformed arrays from (DVC-tracked)
    train_dir: Path
    val_dir:   Path
    test_dir:  Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)

    @property
    def trained_model_filepath(self) -> Path:
        return self.root_dir / self.trained_model_filename

    @property
    def training_report_filepath(self) -> Path:
        return self.root_dir / self.training_report_filename


@dataclass
class ModelEvaluationConfig:
    root_dir: Path
    evaluation_report_filename: str
    train_dir: Path
    val_dir: Path
    test_dir: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)

    @property
    def evaluation_report_filepath(self) -> Path:
        return self.root_dir / self.evaluation_report_filename


@dataclass
class ModelPusherConfig:
    pushed_model_dir: Path
    pushed_model_filename: str
    upload_to_s3: bool

    def __post_init__(self):
        self.pushed_model_dir = Path(self.pushed_model_dir)

    @property
    def pushed_model_filepath(self) -> Path:
        return self.pushed_model_dir / self.pushed_model_filename


@dataclass
class S3HandlerConfig:
    root_dir: Path
    bucket_name: str
    aws_region: str
    local_dir_to_sync: Path
    s3_artifacts_prefix: str
    s3_final_model_prefix: str

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.local_dir_to_sync = Path(self.local_dir_to_sync)

================================================================================
# PY FILE: src\networksecurity\exception\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\exception\exception.py
================================================================================

"""Custom exception and logger interface for the phishing detection web app.

This module defines:
- LoggerInterface: a structural protocol for logging
- NetworkSecurityError: a traceback-aware exception with logging support
"""


import sys
from typing import Protocol


class LoggerInterface(Protocol):
    """A structural interface that defines the logger's required methods.

    Any logger passed to NetworkSecurityException must implement this interface.
    """

    def error(self, message: str) -> None:
        """Log an error-level message."""
        ...


class NetworkSecurityError(Exception):
    """Custom exception class for the phishing detection web application.

    Captures traceback details and logs the error using the provided logger.
    """

    def __init__(self, error_message: Exception, logger: LoggerInterface) -> None:
        """Initialize the exception and log it using the injected logger.

        Args:
            error_message (Exception): The original caught exception.
            logger (LoggerInterface): A logger that supports an `error(str)` method.

        """
        super().__init__(str(error_message))
        self.error_message: str = str(error_message)

        # Get traceback info from sys
        _, _, exc_tb = sys.exc_info()
        self.lineno: int | None = exc_tb.tb_lineno if exc_tb else None
        self.file_name: str | None = (
            exc_tb.tb_frame.f_code.co_filename if exc_tb else "Unknown"
        )

        # Log the formatted error
        logger.error(str(self))

    def __str__(self) -> str:
        """Return a formatted error message with file name and line number."""
        return (
            f"Error occurred in file [{self.file_name}], "
            f"line [{self.lineno}], "
            f"message: [{self.error_message}]"
        )

================================================================================
# PY FILE: src\networksecurity\inference\estimator.py
================================================================================

import joblib
from dataclasses import dataclass
from typing import Any
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


@dataclass
class NetworkModel:
    model: Any
    x_preprocessor: Any = None
    y_preprocessor: Any = None

    def predict(self, X):
        try:
            if self.x_preprocessor:
                X = self.x_preprocessor.transform(X)
            return self.model.predict(X)
        except Exception as e:
            raise NetworkSecurityError(e, logger)

    @classmethod
    def from_artifacts(cls, model_path, x_preprocessor_path=None, y_preprocessor_path=None):
        try:
            model = joblib.load(model_path)

            x_preprocessor = joblib.load(x_preprocessor_path) if x_preprocessor_path else None
            y_preprocessor = joblib.load(y_preprocessor_path) if y_preprocessor_path else None

            return cls(model=model, x_preprocessor=x_preprocessor, y_preprocessor=y_preprocessor)

        except Exception as e:
            raise NetworkSecurityError(e, logger)

    @classmethod
    def from_objects(cls, model, x_preprocessor=None, y_preprocessor=None):
        try:
            return cls(model=model, x_preprocessor=x_preprocessor, y_preprocessor=y_preprocessor)
        except Exception as e:
            raise NetworkSecurityError(e, logger)

================================================================================
# PY FILE: src\networksecurity\logging\__init__.py
================================================================================

"""
Initialize centralized logger for the `networksecurity.logging` package.

This module sets up a reusable logger instance (`logger`) that can be imported
across the project to ensure consistent, centralized logging configuration.

Supports:
- Shared UTC timestamp for file/folder naming
- Dynamic logger name via environment variable
- Dynamic log level via environment variable
"""

import os
import logging

from .logger import setup_logger

# Use env variable for logger name, default to "networksecurity"
LOGGER_NAME = os.getenv("LOGGER_NAME", "networksecurity")

# Use env variable for log level, default to "DEBUG"
LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG").upper()

# Initialize and configure the logger
logger = setup_logger(name=LOGGER_NAME)
logger.setLevel(getattr(logging, LOG_LEVEL, logging.DEBUG))

================================================================================
# PY FILE: src\networksecurity\logging\logger.py
================================================================================

"""
Logging utility module.

Provides `setup_logger()` to configure a logger with both a file and stream handler,
using a UTC timestamp synchronized across the pipeline (logs, artifacts, models, etc.).
"""

import logging
import sys
from pathlib import Path

from src.networksecurity.constants.constants import LOGS_ROOT
from src.networksecurity.utils.timestamp import get_shared_utc_timestamp

def setup_logger(name: str = "app_logger") -> logging.Logger:
    """
    Set up and return a logger instance with a consistent timestamped log directory and file.

    Ensures:
    - One timestamp per pipeline run (shared with ConfigurationManager)
    - No duplicate handlers on repeated setup
    - Clean log formatting to stdout and file

    Args:
        name (str): The name of the logger instance (e.g., 'data_ingestion').

    Returns:
        logging.Logger: Configured logger with file and stream handlers.
    """
    sys.stdout.reconfigure(encoding='utf-8')

    # Get shared timestamp for this run
    timestamp = get_shared_utc_timestamp()

    # Log folder: logs/<timestamp>/
    log_dir = Path(LOGS_ROOT) / timestamp
    log_dir.mkdir(parents=True, exist_ok=True)

    # Log file: logs/<timestamp>/<timestamp>.log
    log_filepath = log_dir / f"{timestamp}.log"

    # Log message format
    log_format = "[%(asctime)s] - %(levelname)s - %(module)s - %(message)s"
    formatter = logging.Formatter(log_format)

    # Get or create logger
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)

    # Add file handler if not already added
    if not any(isinstance(h, logging.FileHandler) and h.baseFilename == str(log_filepath)
               for h in logger.handlers):
        file_handler = logging.FileHandler(log_filepath, mode="a")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    # Add stdout stream handler if not already added
    if not any(isinstance(h, logging.StreamHandler) and h.stream == sys.stdout
               for h in logger.handlers):
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)

    return logger


# Example usage
if __name__ == "__main__":
    logger = setup_logger()
    logger.info("Logger initialized successfully.")

================================================================================
# PY FILE: src\networksecurity\pipeline\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\pipeline\data_ingestion_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_ingestion import DataIngestion
from src.networksecurity.dbhandler.mongodb_handler import MongoDBHandler
from src.networksecurity.entity.config_entity import DataIngestionConfig, MongoHandlerConfig
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataIngestionPipeline:
    """
    Runs the data ingestion stage:
    - Loads configs
    - Instantiates handler and component
    - Triggers ingestion and returns artifact
    """

    def __init__(self):
        try:
            self.config_manager = ConfigurationManager()
            self.ingestion_config: DataIngestionConfig = self.config_manager.get_data_ingestion_config()
            self.mongo_config: MongoHandlerConfig = self.config_manager.get_mongo_handler_config()
            self.mongo_handler = MongoDBHandler(config=self.mongo_config)
        except Exception as e:
            logger.exception("Failed to initialize DataIngestionPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> DataIngestionArtifact:
        try:
            logger.info("========== Data Ingestion Stage Started ==========")

            ingestion = DataIngestion(
                config=self.ingestion_config,
                db_handler=self.mongo_handler,
            )
            artifact = ingestion.run_ingestion()

            logger.info(f"Data Ingestion Process Completed.\n{artifact}")
            logger.info("========== Data Ingestion Stage Completed ==========")

            return artifact

        except Exception as e:
            logger.exception("Data Ingestion Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\data_transformation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_transformation import DataTransformation
from src.networksecurity.entity.artifact_entity import (
    DataValidationArtifact,
    DataTransformationArtifact,
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataTransformationPipeline:
    """
    Orchestrates the Data Transformation stage of the pipeline.

    Responsibilities:
    - Loads transformation configuration
    - Accepts validated artifact
    - Performs feature transformation and returns transformation artifact
    """

    def __init__(self, validation_artifact: DataValidationArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.config = self.config_manager.get_data_transformation_config()
            self.validation_artifact = validation_artifact
        except Exception as e:
            logger.exception("Failed to initialize DataTransformationPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> DataTransformationArtifact:
        try:
            logger.info("========== Data Transformation Stage Started ==========")

            transformer = DataTransformation(
                config=self.config,
                validation_artifact=self.validation_artifact,
            )
            transformation_artifact = transformer.run_transformation()

            logger.info(f"Data Transformation Completed Successfully: {transformation_artifact}")
            logger.info("========== Data Transformation Stage Completed ==========")

            return transformation_artifact

        except Exception as e:
            logger.exception("Data Transformation Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\data_validation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_validation import DataValidation
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataValidationPipeline:
    """
    Orchestrates the Data Validation stage of the pipeline.

    Responsibilities:
    - Fetches the configuration and artifacts
    - Loads validated DataFrame from artifact
    - Validates schema and schema hash
    """

    def __init__(self, ingestion_artifact: DataIngestionArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.ingestion_artifact = ingestion_artifact
            self.config = self.config_manager.get_data_validation_config()
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run(self):
        try:
            logger.info("========= Data Validation Stage Started =========")
            validation = DataValidation(config=self.config, ingestion_artifact=self.ingestion_artifact)
            validation_artifact = validation.run_validation()
            logger.info(f"Data Validation Process Completed.\n{validation_artifact}")
            logger.info("========= Data Validation Stage Completed =========")
            return validation_artifact
        except Exception as e:
            logger.error("Data Validation Pipeline Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\model_evaluation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_evaluation import ModelEvaluation
from src.networksecurity.entity.artifact_entity import (
    ModelTrainerArtifact,
    ModelEvaluationArtifact,
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError

class ModelEvaluationPipeline:
    """
    Orchestrates the Model Evaluation stage of the pipeline.

    Responsibilities:
    - Loads model evaluation configuration
    - Accepts only the trainer artifact (no transformation artifact needed)
    - Evaluates trained model on train/val/test datasets
    - Emits a ModelEvaluationArtifact
    """

    def __init__(self, trainer_artifact: ModelTrainerArtifact):
        try:
            logger.info("Initializing ModelEvaluationPipeline...")
            self.config = ConfigurationManager().get_model_evaluation_config()
            self.trainer_artifact = trainer_artifact
        except Exception as e:
            logger.exception("Failed to initialize ModelEvaluationPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelEvaluationArtifact:
        try:
            logger.info("========== Model Evaluation Stage Started ==========")

            evaluator = ModelEvaluation(
                config=self.config,
                trainer_artifact=self.trainer_artifact
            )
            evaluation_artifact = evaluator.run_evaluation()

            logger.info(f"Model Evaluation Completed Successfully: {evaluation_artifact}")
            logger.info("========== Model Evaluation Stage Completed ==========")

            return evaluation_artifact

        except Exception as e:
            logger.exception("Model Evaluation Stage Failed")
            raise NetworkSecurityError(e, logger) from e
