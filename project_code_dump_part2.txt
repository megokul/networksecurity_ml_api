
ðŸ“¦ Project Structure of: E:\MyProjects\networksecurity

ðŸ“ .dvc/
    ðŸ“„ .gitignore
    ðŸ“ cache/
        ðŸ“ files/
            ðŸ“ md5/
                ðŸ“ 68/
                    ðŸ“„ 770e0efceb9cd3777502f505ecbec4
                ðŸ“ 70/
                    ðŸ“„ 3506878baa6bf0360b2b42782e9677
                ðŸ“ 83/
                    ðŸ“„ de4e3a0dbe7d93403a8c15753377f7
                ðŸ“ 86/
                    ðŸ“„ 4e5c8474fcd1b15bc434932036628d
                ðŸ“ 87/
                    ðŸ“„ f220430be73d16609bfd637b1644af
                ðŸ“ 8d/
                    ðŸ“„ e71e17a20f71bece7659a34bae7027
                ðŸ“ bf/
                    ðŸ“„ fa4350e8aa16caaa3cbb92ec8e048c
                ðŸ“ e1/
                    ðŸ“„ a8ae356f072436dd3acc1bcbafbd89
    ðŸ“„ config
    ðŸ“ tmp/
        ðŸ“„ btime
        ðŸ“„ dag.md
        ðŸ“ exps/
            ðŸ“ cache/
                ðŸ“ 08/
                    ðŸ“„ ba3af380fc7743bb7784b0b99b9e84b597659c
                ðŸ“ 0c/
                    ðŸ“„ 7b66bd77f8e71f84c28f8868f79d1c391b243e
                ðŸ“ 17/
                    ðŸ“„ a15f92afbc3a237675719f1bb52d4e45b2855f
                ðŸ“ 20/
                    ðŸ“„ 418b499ec78701cefc3c979011ef77fa37084d
                ðŸ“ 28/
                    ðŸ“„ 1710634e34c59aaca7c823b82ba97e8c3ae6c8
                    ðŸ“„ e888b2e08a4b7dc34604b66abd2af2aaed6b86
                ðŸ“ 29/
                    ðŸ“„ 28b4f351c2d8f210468c693d53455c7b09a1eb
                    ðŸ“„ dfeb7151d1e0fc143507ecdecb1bedd2b5cc4d
                ðŸ“ 2e/
                    ðŸ“„ d0f21b70c06ff15b188bb69f6ce389a869af78
                ðŸ“ 32/
                    ðŸ“„ 9b746e5971f793e269ced861c2d1e401f43d48
                ðŸ“ 3e/
                    ðŸ“„ a84c7b28b0242174e45334d5d6a9d3026f546d
                ðŸ“ 48/
                    ðŸ“„ faf510268751ce441d9178a563f5e2c4577d88
                ðŸ“ 4a/
                    ðŸ“„ 640ce33b51ffeb8c1aa85e520ce78dfe105123
                ðŸ“ 4c/
                    ðŸ“„ 0bc699b6997f940ba6b39f9c3f002d74b0cd52
                ðŸ“ 51/
                    ðŸ“„ a289cbadb16786fe46c2b4bab48853efc637a9
                ðŸ“ 52/
                    ðŸ“„ 0a8ade47b7e748a46de7cd94c1f1db7a925527
                    ðŸ“„ b49095c9d6e6a69ba58f77d1979c4626e30907
                ðŸ“ 61/
                    ðŸ“„ a81d71c590a7a662176a82aa5f00a7532a3e16
                ðŸ“ 63/
                    ðŸ“„ 02ec043a14cb6881a1196ca1b58f7df093a586
                    ðŸ“„ b517b2eff95eb91aba4bfef87abcd7955ae01c
                ðŸ“ 64/
                    ðŸ“„ 3172fd8ac68c1fe35ae5f9a8b4734633a30778
                ðŸ“ 67/
                    ðŸ“„ fee62ed0bb527f9980ea136f0c312784242b38
                ðŸ“ 68/
                    ðŸ“„ 81874523aeb9b0b77eb7bae250d0287e341558
                ðŸ“ 6c/
                    ðŸ“„ 4d54822a8eec14b0c0ea4846ba14c9541e31dd
                ðŸ“ 7f/
                    ðŸ“„ e668c86a64d0171bc4dc8708460ac48ec77a34
                ðŸ“ 88/
                    ðŸ“„ a8f98863afa828023b2e3616348335b6d4628e
                ðŸ“ 89/
                    ðŸ“„ daed7059804441e9041b6d073c071a1cfcbe2c
                ðŸ“ 8a/
                    ðŸ“„ a79a390fba4858401f3e79d94bee5dbf354341
                    ðŸ“„ ed08439cb37bbee9fee4ad00d82ea30435606b
                ðŸ“ a5/
                    ðŸ“„ 168ff7e9460e593e744f60ef1bad04413e1ca7
                ðŸ“ a8/
                    ðŸ“„ 2c1cb5bbc048ca32cb78061a4b8aaf3735233f
                ðŸ“ ad/
                    ðŸ“„ 329b2130c11421d465202bdf74f5170e661c23
                ðŸ“ af/
                    ðŸ“„ ea20d43951b4448cd0a4eb4a7a134958cfb5c5
                ðŸ“ b0/
                    ðŸ“„ 63569211a95f11f23c4f61f6b9518bbf3b2cf3
                ðŸ“ b2/
                    ðŸ“„ a55eaf06589f37339db03046e528a4eb70f795
                ðŸ“ bb/
                    ðŸ“„ 6504b5b79c287b4b60d2355251d8b30025c769
                    ðŸ“„ e1aa1f0748e2a41f3ece907bfaaae1cafb5c48
                ðŸ“ c4/
                    ðŸ“„ 9033287ca54d77ad6ab9c7603c06825bbac4e6
                ðŸ“ c7/
                    ðŸ“„ 31a093cb23b7a2bef65922d39d9d20ac56e6f3
                ðŸ“ c8/
                    ðŸ“„ dbdb66a143e984e4fe8b43c7d755441cdec3f5
                ðŸ“ ce/
                    ðŸ“„ 1c26eef879f92adf044d9d0da37c028fd7d494
                    ðŸ“„ 4e08484f42af33f1129dda5084bf3fed271a22
                ðŸ“ d2/
                    ðŸ“„ 3e801f5cac5d3a86a0e23a6fdbe1cbe281f789
                ðŸ“ d6/
                    ðŸ“„ 4f71f9b0de7fb7fca7d5f21249de0a489b4ca8
                ðŸ“ dc/
                    ðŸ“„ fff6f4ba5cf77e9a0eaf75c0830d9795bb58cf
                ðŸ“ dd/
                    ðŸ“„ ee659d7114460f19e1804a98117314724c9b04
                ðŸ“ e3/
                    ðŸ“„ 5de325a2125360b5ae289c080fc748a0140cdc
                    ðŸ“„ 8f209c48f47e7a864c8c9c61048ce16f1a2896
                ðŸ“ eb/
                    ðŸ“„ 79e8cfe8c4a974d876650c1a48de56761ef2a1
                ðŸ“ f3/
                    ðŸ“„ 8d3509705265cc0399a9e13b577074bfb9aedf
                ðŸ“ f6/
                    ðŸ“„ f9d61771a2404a0d798efa45ff7ff15c8f639a
                ðŸ“ f9/
                    ðŸ“„ 21cb7aa3662a13f16a67e07a71781cec9b1a91
            ðŸ“ celery/
                ðŸ“ broker/
                    ðŸ“ control/
                    ðŸ“ in/
                    ðŸ“ processed/
                ðŸ“ result/
        ðŸ“„ lock
        ðŸ“„ rwlock
        ðŸ“„ rwlock.lock
ðŸ“„ .dvcignore
ðŸ“„ .env
ðŸ“„ .gitignore
ðŸ“„ Dockerfile
ðŸ“„ LICENSE
ðŸ“ NetworkSecurity.egg-info/
    ðŸ“„ PKG-INFO
    ðŸ“„ SOURCES.txt
    ðŸ“„ dependency_links.txt
    ðŸ“„ requires.txt
    ðŸ“„ top_level.txt
ðŸ“„ README.md
ðŸ“„ app.py
ðŸ“ artifacts/
    ðŸ“ 2025_05_24T09_51_21Z/
        ðŸ“ data_ingestion/
            ðŸ“ featurestore/
                ðŸ“„ raw_data.csv
            ðŸ“ ingested/
                ðŸ“„ ingested_data.csv
        ðŸ“ data_transformation/
            ðŸ“ preprocessor/
                ðŸ“„ x_preprocessor.joblib
                ðŸ“„ y_preprocessor.joblib
            ðŸ“ transformed/
                ðŸ“ test/
                    ðŸ“„ x_test.npy
                    ðŸ“„ y_test.npy
                ðŸ“ train/
                    ðŸ“„ x_train.npy
                    ðŸ“„ y_train.npy
                ðŸ“ val/
                    ðŸ“„ x_val.npy
                    ðŸ“„ y_val.npy
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.json
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
        ðŸ“ model_evaluation/
            ðŸ“„ evaluation_report.yaml
        ðŸ“ model_trainer/
            ðŸ“ inference_model/
                ðŸ“„ inference_model.joblib
            ðŸ“ reports/
                ðŸ“„ training_report.yaml
            ðŸ“ trained_model/
                ðŸ“„ model.joblib
ðŸ“ config/
    ðŸ“„ config.yaml
    ðŸ“„ params.yaml
    ðŸ“„ schema.yaml
    ðŸ“„ templates.yaml
ðŸ“ data/
    ðŸ“ raw/
        ðŸ“„ raw_data.csv
        ðŸ“„ raw_data.csv.dvc
    ðŸ“ transformed/
        ðŸ“ test/
            ðŸ“„ x_test.npy
            ðŸ“„ x_test.npy.dvc
            ðŸ“„ y_test.npy
            ðŸ“„ y_test.npy.dvc
        ðŸ“ train/
            ðŸ“„ x_train.npy
            ðŸ“„ x_train.npy.dvc
            ðŸ“„ y_train.npy
            ðŸ“„ y_train.npy.dvc
        ðŸ“ val/
            ðŸ“„ x_val.npy
            ðŸ“„ x_val.npy.dvc
            ðŸ“„ y_val.npy
            ðŸ“„ y_val.npy.dvc
    ðŸ“ validated/
        ðŸ“„ validated_data.csv
        ðŸ“„ validated_data.csv.dvc
ðŸ“„ debug.py
ðŸ“„ debug_pipeline.py
ðŸ“„ docker-compose.yaml
ðŸ“ final_model/
    ðŸ“„ final_inference_model.joblib
ðŸ“ logs/
    ðŸ“ 2025_05_24T09_51_21Z/
        ðŸ“„ 2025_05_24T09_51_21Z.log
ðŸ“„ main.py
ðŸ“ network_data/
    ðŸ“ input_csv/
        ðŸ“„ phisingData.csv
        ðŸ“„ phisingData_check.csv
ðŸ“ prediction_output/
ðŸ“„ print_structure.py
ðŸ“„ project_dump.py
ðŸ“„ project_template.py
ðŸ“„ requirements.txt
ðŸ“ research/
    ðŸ“ config/
        ðŸ“„ schema.yaml
    ðŸ“„ ingested_data.csv
    ðŸ“„ research.ipynb
ðŸ“„ setup.py
ðŸ“ src/
    ðŸ“ networksecurity/
        ðŸ“„ __init__.py
        ðŸ“ components/
            ðŸ“„ __init__.py
            ðŸ“„ data_ingestion.py
            ðŸ“„ data_transformation.py
            ðŸ“„ data_validation.py
            ðŸ“„ model_evaluation.py
            ðŸ“„ model_pusher.py
            ðŸ“„ model_trainer.py
        ðŸ“ config/
            ðŸ“„ __init__.py
            ðŸ“„ configuration.py
        ðŸ“ constants/
            ðŸ“„ __init__.py
            ðŸ“„ constants.py
        ðŸ“ data_processors/
            ðŸ“„ encoder_factory.py
            ðŸ“„ imputer_factory.py
            ðŸ“„ label_mapper.py
            ðŸ“„ preprocessor_builder.py
            ðŸ“„ scaler_factory.py
        ðŸ“ dbhandler/
            ðŸ“„ __init__.py
            ðŸ“„ base_handler.py
            ðŸ“„ mongodb_handler.py
            ðŸ“„ s3_handler.py
        ðŸ“ entity/
            ðŸ“„ __init__.py
            ðŸ“„ artifact_entity.py
            ðŸ“„ config_entity.py
        ðŸ“ exception/
            ðŸ“„ __init__.py
            ðŸ“„ exception.py
        ðŸ“ inference/
            ðŸ“„ estimator.py
        ðŸ“ logging/
            ðŸ“„ __init__.py
            ðŸ“„ logger.py
        ðŸ“ pipeline/
            ðŸ“„ __init__.py
            ðŸ“„ data_ingestion_pipeline.py
            ðŸ“„ data_transformation_pipeline.py
            ðŸ“„ data_validation_pipeline.py
            ðŸ“„ model_evaluation_pipeline.py
            ðŸ“„ model_pusher_pipeline.py
            ðŸ“„ model_trainer_pipeline.py
            ðŸ“„ training_pipeline.py
        ðŸ“ utils/
            ðŸ“„ __init__.py
            ðŸ“„ core.py
            ðŸ“„ timestamp.py
        ðŸ“ worker/
            ðŸ“„ celery_worker.py
ðŸ“ templates/
    ðŸ“„ table.html

--- CODE DUMP | PART 2 of 4 ---


================================================================================
# PY FILE: src\networksecurity\components\model_pusher.py
================================================================================

from src.networksecurity.entity.config_entity import ModelPusherConfig
from src.networksecurity.entity.artifact_entity import ModelTrainerArtifact, ModelPusherArtifact
from src.networksecurity.dbhandler.s3_handler import S3Handler
from src.networksecurity.utils.core import save_object, load_object
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from pathlib import Path


class ModelPusher:
    """
    Pushes the final trained model to local storage and optionally to S3.
    """

    def __init__(
        self,
        model_pusher_config: ModelPusherConfig,
        model_trainer_artifact: ModelTrainerArtifact,
        s3_handler: S3Handler,
    ) -> None:
        try:
            self.config = model_pusher_config
            self.trainer_artifact = model_trainer_artifact
            self.s3_handler = s3_handler
        except Exception as e:
            logger.exception("Failed to initialize ModelPusher")
            raise NetworkSecurityError(e, logger) from e

    def push_model(self) -> ModelPusherArtifact:
        try:
            logger.info("Starting model push process...")

            final_model = load_object(self.trainer_artifact.trained_model_filepath)

            # Save model locally
            save_object(
                obj=final_model,
                path=self.config.pushed_model_filepath,
                label="Final Model",
            )

            if self.config.upload_to_s3:
                with self.s3_handler as handler:

                    s3_key_final_model = handler.config.s3_final_model_prefix + "/" + self.config.pushed_model_filename

                    # Upload only the final model file
                    handler.upload_file(
                        local_path=self.config.pushed_model_filepath,
                        s3_key=s3_key_final_model,
                    )

                    s3_key_artifacts = handler.config.s3_artifacts_prefix + "/artifacts"

                    # Upload entire artifacts folder
                    handler.sync_directory(
                        local_dir=Path("artifacts"),
                        s3_prefix=s3_key_artifacts,
                    )

                    # Upload entire logs folder
                    handler.sync_directory(
                        local_dir=Path("logs"),
                        s3_prefix=handler.config.s3_artifacts_prefix + "/logs"
                    )

            logger.info("Model push process completed successfully.")
            return ModelPusherArtifact(
                pushed_model_local_path=self.config.pushed_model_filepath,
                pushed_model_s3_path=f"s3://{self.s3_handler.config.bucket_name}/{s3_key_final_model}",
            )

        except Exception as e:
            logger.exception("Model push failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\components\model_trainer.py
================================================================================

from pathlib import Path
from datetime import datetime, timezone
import os
import importlib
import numpy as np
import optuna
import mlflow
import dagshub
import joblib
from sklearn.model_selection import cross_val_score
from sklearn.metrics import get_scorer
from mlflow import sklearn as mlflow_sklearn

from src.networksecurity.entity.config_entity import ModelTrainerConfig
from src.networksecurity.entity.artifact_entity import DataTransformationArtifact, ModelTrainerArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.utils.core import save_to_yaml, save_object, load_array
from src.networksecurity.inference.estimator import NetworkModel


class ModelTrainer:
    def __init__(self, config: ModelTrainerConfig, transformation_artifact: DataTransformationArtifact):
        try:
            self.config = config
            self.transformation_artifact = transformation_artifact
            logger.info(f"Initializing ModelTrainer with root_dir={config.root_dir}")
            config.root_dir.mkdir(parents=True, exist_ok=True)

            if config.tracking.mlflow.enabled:
                dagshub.init(
                    repo_owner=os.getenv("DAGSHUB_REPO_OWNER"),
                    repo_name=os.getenv("DAGSHUB_REPO_NAME"),
                    mlflow=True,
                )
                mlflow.set_tracking_uri(config.tracking.tracking_uri)
                mlflow.set_experiment(config.tracking.mlflow.experiment_name)
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _load_data(self):
        try:
            self.X_train = load_array(self.transformation_artifact.x_train_filepath, "X train")
            self.y_train = load_array(self.transformation_artifact.y_train_filepath, "Y train")
            self.X_val = load_array(self.transformation_artifact.x_val_filepath, "X val")
            self.y_val = load_array(self.transformation_artifact.y_val_filepath, "Y val")
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _instantiate(self, full_class_string: str, params: dict):
        module_path, class_name = full_class_string.rsplit(".", 1)
        module = importlib.import_module(module_path)
        return getattr(module, class_name)(**(params or {}))

    def _optimize_one(self, model_spec: dict):
        model_name = model_spec["name"]
        search_space = model_spec.get("search_space", {})

        def objective(trial):
            sampled = {}
            for name, space in search_space.items():
                if "choices" in space:
                    sampled[name] = trial.suggest_categorical(name, space["choices"])
                else:
                    low, high = space["low"], space["high"]
                    step = space.get("step", 1)
                    log = space.get("log", False)
                    if isinstance(low, int) and isinstance(high, int):
                        sampled[name] = trial.suggest_int(name, low, high, step=step)
                    else:
                        sampled[name] = trial.suggest_float(name, float(low), float(high), log=log)
            clf = self._instantiate(model_name, sampled)
            scores = cross_val_score(clf, self.X_train, self.y_train, cv=self.config.optimization.cv_folds,
                                     scoring=self.config.optimization.scoring, n_jobs=-1)
            return scores.mean()

        study = optuna.create_study(direction=self.config.optimization.direction)
        study.optimize(objective, n_trials=self.config.optimization.n_trials)
        return study.best_trial, study

    def _select_and_tune(self):
        best_result = {"score": -np.inf, "spec": None, "trial": None, "study": None}

        for model_spec in self.config.models:
            if self.config.optimization.enabled:
                trial, study = self._optimize_one(model_spec)
                score = trial.value
            else:
                model = self._instantiate(model_spec["name"], model_spec.get("params", {}))
                score = cross_val_score(model, self.X_train, self.y_train, cv=self.config.optimization.cv_folds,
                                        scoring=self.config.optimization.scoring).mean()
                trial, study = None, None

            if score > best_result["score"]:
                best_result.update(score=score, spec=model_spec, trial=trial, study=study)

        return best_result

    def _train_and_eval(self, model_spec: dict, params: dict):
        clf = self._instantiate(model_spec["name"], params)
        clf.fit(self.X_train, self.y_train)

        train_metrics = {m: get_scorer(m)(clf, self.X_train, self.y_train)
                         for m in self.config.tracking.mlflow.metrics_to_log}
        val_metrics = {m: get_scorer(m)(clf, self.X_val, self.y_val)
                       for m in self.config.tracking.mlflow.metrics_to_log}
        return clf, train_metrics, val_metrics

    def _generate_report(self, best, train_metrics, val_metrics):
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "best_model": best["spec"]["name"].split(".")[-1],
            "best_model_params": best["trial"].params if best["trial"] else best["spec"].get("params", {}),
            "train_metrics": train_metrics,
            "val_metrics": val_metrics,
            "optimization": {
                "enabled": self.config.optimization.enabled,
                "best_trial": best["trial"].number if best["trial"] else None,
                "cv_folds": self.config.optimization.cv_folds,
                "direction": self.config.optimization.direction,
                "mean_score": best["score"]
            }
        }

    def run_training(self) -> ModelTrainerArtifact:
        try:
            logger.info("========== Starting Model Training ==========")
            self._load_data()

            with mlflow.start_run():
                best = self._select_and_tune()
                params = best["trial"].params if best["trial"] else best["spec"].get("params", {})
                model, train_m, val_m = self._train_and_eval(best["spec"], params)

                mlflow.log_params(params)
                for k, v in train_m.items():
                    mlflow.log_metric(f"train_{k}", v)
                for k, v in val_m.items():
                    mlflow.log_metric(f"val_{k}", v)

                # Save raw trained model
                trained_model_dir = self.config.root_dir / "trained_model"
                trained_model_path = trained_model_dir / "model.joblib"
                save_object(model, trained_model_path, "Trained Model")

                # Save inference-ready model (NetworkModel)
                network_model = NetworkModel.from_objects(
                    model=model,
                    x_preprocessor=joblib.load(self.transformation_artifact.x_preprocessor_filepath),
                    y_preprocessor=joblib.load(self.transformation_artifact.y_preprocessor_filepath),
                )
                inference_model_dir = self.config.root_dir / "inference_model"
                inference_model_path = inference_model_dir / "inference_model.joblib"
                save_object(network_model, inference_model_path, "Inference Model")

                # Save training report
                report_dir = self.config.root_dir / "reports"
                report_path = report_dir / "training_report.yaml"
                save_to_yaml(self._generate_report(best, train_m, val_m), report_path, label="Training Report")

            logger.info("========== Model Training Completed ==========")
            return ModelTrainerArtifact(
                trained_model_filepath=inference_model_path,
                training_report_filepath=report_path,
                x_train_filepath=self.transformation_artifact.x_train_filepath,
                y_train_filepath=self.transformation_artifact.y_train_filepath,
                x_val_filepath=self.transformation_artifact.x_val_filepath,
                y_val_filepath=self.transformation_artifact.y_val_filepath,
                x_test_filepath=self.transformation_artifact.x_test_filepath,
                y_test_filepath=self.transformation_artifact.y_test_filepath,
            )

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\config\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\config\configuration.py
================================================================================

from pathlib import Path
import os

from src.networksecurity.constants.constants import (
    CONFIG_FILE_PATH,
    PARAMS_FILE_PATH,
    SCHEMA_FILE_PATH,
    TEMPLATES_FILE_PATH,
    MONGO_HANDLER_SUBDIR,
    MONGO_JSON_SUBDIR,
    DATA_INGESTION_SUBDIR,
    FEATURESTORE_SUBDIR,
    INGESTED_SUBDIR,
    DATA_VALIDATION_SUBDIR,
    VALIDATED_SUBDIR,
    REPORTS_SUBDIR,
    DATA_TRANSFORMATION_SUBDIR,
    TRANSFORMED_DATA_SUBDIR,
    DATA_TRAIN_SUBDIR,
    DATA_VAL_SUBDIR,
    DATA_TEST_SUBDIR,
    TRANSFORMED_OBJECT_SUBDIR,
    LOGS_ROOT,
    MODEL_TRAINER_SUBDIR,
    MODEL_EVALUATION_SUBDIR,
    PUSHED_MODEL_SUBDIR,
)

from src.networksecurity.entity.config_entity import (
    MongoHandlerConfig,
    DataIngestionConfig,
    DataValidationConfig,
    DataTransformationConfig,
    ModelTrainerConfig,
    ModelEvaluationConfig,
    ModelPusherConfig,
    S3HandlerConfig,
)

from src.networksecurity.utils.core import (
    read_yaml,
    replace_username_password_in_uri,
)
from src.networksecurity.utils.timestamp import get_shared_utc_timestamp
from src.networksecurity.logging import logger


class ConfigurationManager:
    """
    Loads YAML-based configuration and generates paths for all pipeline stages.
    Dynamically sets timestamped artifact directories per run.
    """
    _global_timestamp: str = None

    def __init__(
        self,
        config_filepath: Path = CONFIG_FILE_PATH,
        params_filepath: Path = PARAMS_FILE_PATH,
        schema_filepath: Path = SCHEMA_FILE_PATH,
        templates_filepath: Path = TEMPLATES_FILE_PATH,
    ) -> None:
        self._load_configs(config_filepath, params_filepath, schema_filepath, templates_filepath)
        self._initialize_paths()

    def _load_configs(self, config_filepath: Path, params_filepath: Path, schema_filepath: Path, templates_filepath: Path):
        self.config = read_yaml(config_filepath)
        self.params = read_yaml(params_filepath)
        self.schema = read_yaml(schema_filepath)
        self.templates = read_yaml(templates_filepath)

    def _initialize_paths(self) -> None:
        if ConfigurationManager._global_timestamp is None:
            ConfigurationManager._global_timestamp = get_shared_utc_timestamp()

        timestamp = ConfigurationManager._global_timestamp
        base_artifact_root = Path(self.config.project.artifacts_root)
        self.artifacts_root = base_artifact_root / timestamp

        self.logs_root = Path(LOGS_ROOT) / timestamp

    def get_logs_dir(self) -> Path:
        return self.logs_root

    def get_artifact_root(self) -> Path:
        return self.artifacts_root

    def get_mongo_handler_config(self) -> MongoHandlerConfig:
        mongo_cfg = self.config.mongo_handler
        root_dir = self.artifacts_root / MONGO_HANDLER_SUBDIR
        json_data_dir = root_dir / MONGO_JSON_SUBDIR

        mongodb_uri = replace_username_password_in_uri(
            base_uri=os.getenv("MONGODB_URI_BASE"),
            username=os.getenv("MONGODB_USERNAME"),
            password=os.getenv("MONGODB_PASSWORD"),
        )

        return MongoHandlerConfig(
            root_dir=root_dir,
            input_data_path=Path(mongo_cfg.input_data_path),
            json_data_filename=mongo_cfg.json_data_filename,
            json_data_dir=json_data_dir,
            mongodb_uri=mongodb_uri,
            database_name=mongo_cfg.database_name,
            collection_name=mongo_cfg.collection_name,
        )

    def get_data_ingestion_config(self) -> DataIngestionConfig:
        ingestion_cfg = self.config.data_ingestion
        root_dir = self.artifacts_root / DATA_INGESTION_SUBDIR
        featurestore_dir = root_dir / FEATURESTORE_SUBDIR
        ingested_data_dir = root_dir / INGESTED_SUBDIR

        raw_dvc_path = Path(self.config.data_paths.raw_data_dvc_filepath)

        return DataIngestionConfig(
            root_dir=root_dir,
            featurestore_dir=featurestore_dir,
            raw_data_filename=ingestion_cfg.raw_data_filename,
            ingested_data_dir=ingested_data_dir,
            ingested_data_filename=ingestion_cfg.ingested_data_filename,
            raw_dvc_path=raw_dvc_path,
        )

    def get_data_validation_config(self) -> DataValidationConfig:
        validation_cfg = self.config.data_validation
        root_dir = self.artifacts_root / DATA_VALIDATION_SUBDIR
        validated_dir = root_dir / VALIDATED_SUBDIR
        report_dir = root_dir / REPORTS_SUBDIR

        validated_dvc_path = Path(self.config.data_paths.validated_dvc_filepath)

        return DataValidationConfig(
            root_dir=root_dir,
            validated_dir=validated_dir,
            validated_filename=validation_cfg.validated_filename,
            report_dir=report_dir,
            missing_report_filename=validation_cfg.missing_report_filename,
            duplicates_report_filename=validation_cfg.duplicates_report_filename,
            drift_report_filename=validation_cfg.drift_report_filename,
            validation_report_filename=validation_cfg.validation_report_filename,
            schema=self.schema,
            validated_dvc_path=validated_dvc_path,
            validation_params=self.params.validation_params,
            val_report_template=self.templates.validation_report
        )

    def get_data_transformation_config(self) -> DataTransformationConfig:
        transformation_cfg = self.config.data_transformation
        transformation_params = self.params.transformation_params
        target_column = self.schema.target_column

        root_dir = self.artifacts_root / DATA_TRANSFORMATION_SUBDIR
        train_dir = root_dir / TRANSFORMED_DATA_SUBDIR / DATA_TRAIN_SUBDIR
        val_dir = root_dir / TRANSFORMED_DATA_SUBDIR / DATA_VAL_SUBDIR
        test_dir = root_dir / TRANSFORMED_DATA_SUBDIR / DATA_TEST_SUBDIR
        preprocessor_dir = root_dir / TRANSFORMED_OBJECT_SUBDIR

        train_dvc_dir = Path(self.config.data_paths.train_dvc_dir)
        val_dvc_dir = Path(self.config.data_paths.val_dvc_dir)
        test_dvc_dir = Path(self.config.data_paths.test_dvc_dir)

        return DataTransformationConfig(
            root_dir=root_dir,
            transformation_params=transformation_params,
            train_dir=train_dir,
            val_dir=val_dir,
            test_dir=test_dir,
            target_column=target_column,
            x_train_filename=transformation_cfg.x_train_filename,
            y_train_filename=transformation_cfg.y_train_filename,
            x_val_filename=transformation_cfg.x_val_filename,
            y_val_filename=transformation_cfg.y_val_filename,
            x_test_filename=transformation_cfg.x_test_filename,
            y_test_filename=transformation_cfg.y_test_filename,
            preprocessor_dir=preprocessor_dir,
            x_preprocessor_filename=transformation_cfg.x_preprocessor_filename,
            y_preprocessor_filename=transformation_cfg.y_preprocessor_filename,
            train_dvc_dir=train_dvc_dir,
            val_dvc_dir=val_dvc_dir,
            test_dvc_dir=test_dvc_dir,
        )


    def get_model_trainer_config(self) -> ModelTrainerConfig:
        # static config.yaml entries
        yaml_cfg = self.config.model_trainer
        # dynamic params.yaml entries
        params_cfg = self.params.model_trainer

        # where to write models & reports
        root_dir = self.artifacts_root / MODEL_TRAINER_SUBDIR

        # DVCâ€tracked data dirs (under /data/transformed)
        train_dir = Path(self.config.data_paths.train_dvc_dir)
        val_dir = Path(self.config.data_paths.val_dvc_dir)
        test_dir = Path(self.config.data_paths.test_dvc_dir)

        # pull MLflow subâ€box and inject the URI from env
        mlflow_cfg = params_cfg.tracking
        mlflow_cfg.tracking_uri = os.getenv("MLFLOW_TRACKING_URI")

        return ModelTrainerConfig(
            # where to write artifacts
            root_dir=root_dir,
            trained_model_filename=yaml_cfg.trained_model_filename,
            training_report_filename=yaml_cfg.training_report_filename,

            # what to train & how
            models=params_cfg.models,
            optimization=params_cfg.optimization,
            tracking=mlflow_cfg,

            # where to load transformed data from
            train_dir=train_dir,
            val_dir=val_dir,
            test_dir=test_dir,
        )

    def get_model_evaluation_config(self) -> ModelEvaluationConfig:
        eval_cfg = self.config.model_evaluation
        root_dir = self.artifacts_root / MODEL_EVALUATION_SUBDIR

        train_dir = Path(self.config.data_paths.train_dvc_dir)
        val_dir = Path(self.config.data_paths.val_dvc_dir)
        test_dir = Path(self.config.data_paths.test_dvc_dir)

        return ModelEvaluationConfig(
            root_dir=root_dir,
            evaluation_report_filename=eval_cfg.evaluation_report_filename,
            train_dir=train_dir,
            val_dir=val_dir,
            test_dir=test_dir,
        )

    def get_model_pusher_config(self) -> ModelPusherConfig:
        pusher_cfg = self.config.model_pusher
        pusher_params = self.params.model_pusher

        # Local directory to save pushed model
        pushed_model_dir = Path(PUSHED_MODEL_SUBDIR)

        return ModelPusherConfig(
            pushed_model_filename=pusher_cfg.final_model_filename,
            pushed_model_dir=pushed_model_dir,
            upload_to_s3=pusher_params.upload_to_s3,
        )

    def get_s3_handler_config(self) -> S3HandlerConfig:
        s3_cfg = self.config.s3_handler  # your config.yaml has these under model_pusher
        root_dir = self.artifacts_root / "s3_handler"
        aws_region = os.getenv("AWS_REGION")

        return S3HandlerConfig(
            root_dir=root_dir,
            bucket_name=s3_cfg.final_model_s3_bucket,
            aws_region=aws_region,
            local_dir_to_sync=self.artifacts_root,  # assuming you want to sync entire artifacts dir
            s3_artifacts_prefix=s3_cfg.s3_artifacts_prefix,
            s3_final_model_prefix=s3_cfg.s3_final_model_prefix,
        )

================================================================================
# PY FILE: src\networksecurity\constants\__init__.py
================================================================================

from pathlib import Path

CONFIG_FILE_PATH = Path("config/config.yaml")
PARAMS_FILE_PATH = Path("config/params.yaml")
SCHEMA_FILE_PATH = Path("config/schema.yaml")

================================================================================
# PY FILE: src\networksecurity\constants\constants.py
================================================================================

from pathlib import Path

# ---------------------------
# Configuration File Paths
# ---------------------------

CONFIG_DIR = Path("config")
CONFIG_FILE_PATH = CONFIG_DIR / "config.yaml"
PARAMS_FILE_PATH = CONFIG_DIR / "params.yaml"
SCHEMA_FILE_PATH = CONFIG_DIR / "schema.yaml"
TEMPLATES_FILE_PATH = CONFIG_DIR / "templates.yaml"

# ---------------------------
# Generic Constants
# ---------------------------

MISSING_VALUE_TOKEN = "na"

# ---------------------------
# MongoDB Connection Settings
# ---------------------------

MONGODB_CONNECT_TIMEOUT_MS = 40000
MONGODB_SOCKET_TIMEOUT_MS = 40000

# ---------------------------
# Root Directories
# ---------------------------

LOGS_ROOT = "logs"  # Central log directory (outside artifacts)
STABLE_DATA_DIR = Path("data")
RAW_DATA_SUBDIR = "raw"
VALIDATED_DATA_SUBDIR = "validated"
TRANSFORMED_DATA_SUBDIR = "transformed"
MODEL_DIR = "model"
EVALUATION_DIR = "evaluation"
PREDICTIONS_DIR = "predictions"

# ---------------------------
# Artifact Subdirectory Names (Dynamic Timestamped)
# ---------------------------

MONGO_HANDLER_SUBDIR = "mongo_handler"
MONGO_JSON_SUBDIR = "JSON_data"

DATA_INGESTION_SUBDIR = "data_ingestion"
FEATURESTORE_SUBDIR = "featurestore"
INGESTED_SUBDIR = "ingested"

DATA_VALIDATION_SUBDIR = "data_validation"
VALIDATED_SUBDIR = "validated"
REPORTS_SUBDIR = "reports"
SCHEMA_HASH_SUBDIR = "schema_hash"

DATA_TRANSFORMATION_SUBDIR = "data_transformation"
DATA_SUBDIR = "data"
DATA_TRAIN_SUBDIR = "train"
DATA_VAL_SUBDIR = "val"
DATA_TEST_SUBDIR = "test"

TRANSFORMED_OBJECT_SUBDIR = "preprocessor"

MODEL_TRAINER_SUBDIR = "model_trainer"
MODEL_EVALUATION_SUBDIR = "model_evaluation"
MODEL_PREDICTION_SUBDIR = "model_prediction"

# ---------------------------
# Default Filenames (used by config.yaml)
# ---------------------------

DEFAULT_SCHEMA_HASH_FILENAME = "schema_hash.json"
DEFAULT_VALIDATED_FILENAME = "validated_data.csv"
DEFAULT_MISSING_REPORT_FILENAME = "missing_values_report.json"
DEFAULT_DRIFT_REPORT_FILENAME = "drift_report.yaml"
DEFAULT_VALIDATION_REPORT_FILENAME = "validation_report.yaml"

# Logging and output labels
X_TRAIN_LABEL = "X_train"
Y_TRAIN_LABEL = "y_train"
X_VAL_LABEL = "X_val"
Y_VAL_LABEL = "y_val"
X_TEST_LABEL = "X_test"
Y_TEST_LABEL = "y_test"

MODEL_EVALUATION_SUBDIR = "model_evaluation"

PUSHED_MODEL_SUBDIR = "final_model"

================================================================================
# PY FILE: src\networksecurity\data_processors\encoder_factory.py
================================================================================

from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.pipeline import Pipeline

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class EncoderFactory:
    """
    Factory to build encoding pipelines for categorical features.
    Supports: onehot, ordinal.
    Easily extendable with new encoders.
    """
    _SUPPORTED_METHODS = {
        "onehot": OneHotEncoder,
        "ordinal": OrdinalEncoder
    }

    @staticmethod
    def get_encoder_pipeline(method: str, params: dict = None) -> Pipeline:
        try:
            if method in EncoderFactory._SUPPORTED_METHODS:
                encoder_class = EncoderFactory._SUPPORTED_METHODS[method]
                encoder = encoder_class(**(params or {}))
            else:
                raise ValueError(f"Unsupported encoding method: {method}")

            return Pipeline([("encoder", encoder)])

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\data_processors\imputer_factory.py
================================================================================

from sklearn.experimental import enable_iterative_imputer  # noqa: F401
from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer
from sklearn.pipeline import Pipeline

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class ImputerFactory:
    """
    Factory to generate sklearn-compatible imputer pipelines.
    Supports: knn, simple, iterative, and custom methods.
    Easily extendable with new methods.
    """
    _SUPPORTED_METHODS = {
        "knn": KNNImputer,
        "simple": SimpleImputer,
        "iterative": IterativeImputer,
    }

    @staticmethod
    def get_imputer_pipeline(method: str, params: dict) -> Pipeline:
        try:
            if method == "custom":
                if "custom_callable" not in params:
                    raise ValueError("Custom imputer requires a 'custom_callable' in params.")
                imputer = params["custom_callable"]()
            else:
                imputer_class = ImputerFactory._SUPPORTED_METHODS.get(method)
                if not imputer_class:
                    raise ValueError(f"Unsupported imputation method: {method}")
                imputer = imputer_class(**params)

            return Pipeline([("imputer", imputer)])

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\data_processors\label_mapper.py
================================================================================

from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd


class LabelMapper(BaseEstimator, TransformerMixin):
    """
    Custom transformer to map target labels from one value to another.
    For example, map -1 to 0.
    """
    def __init__(self, from_value, to_value):
        self.from_value = from_value
        self.to_value = to_value

    def fit(self, X, y=None):
        return self

    def transform(self, y):
        if isinstance(y, pd.Series):
            return y.replace(self.from_value, self.to_value)
        elif isinstance(y, pd.DataFrame):
            return y.apply(lambda col: col.replace(self.from_value, self.to_value))
        else:
            raise ValueError("Unsupported data type for label transformation.")

================================================================================
# PY FILE: src\networksecurity\data_processors\preprocessor_builder.py
================================================================================

from sklearn.pipeline import Pipeline
from sklearn.experimental import enable_iterative_imputer
from src.networksecurity.data_processors.imputer_factory import ImputerFactory
from src.networksecurity.data_processors.scaler_factory import ScalerFactory
from src.networksecurity.data_processors.encoder_factory import EncoderFactory
from src.networksecurity.data_processors.label_mapper import LabelMapper
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class PreprocessorBuilder:
    """
    Builds preprocessing pipelines for X and Y using dynamic configuration.
    Skips steps that are explicitly set to "none" or not provided at all.
    """

    STEP_BUILDERS = {
        "imputer": ImputerFactory.get_imputer_pipeline,
        "scaler": ScalerFactory.get_scaler_pipeline,
        "encoder": EncoderFactory.get_encoder_pipeline,
        "label_mapping": lambda method, params: LabelMapper(from_value=params["from"], to_value=params["to"])
    }

    def __init__(self, steps: dict, methods: dict):
        self.steps = steps or {}
        self.methods = methods or {}

    def _build_pipeline(self, section: str) -> Pipeline:
        try:
            pipeline_steps = []
            steps_to_build = self.steps.get(section, {})
            methods = self.methods.get(section, {})

            for step_name, method_name in steps_to_build.items():
                # Skip if explicitly marked as 'none' (case-insensitive)
                if method_name is None or str(method_name).lower() == "none":
                    logger.info(f"Skipping step '{step_name}' for '{section}' as it is set to 'none'.")
                    continue

                builder = self.STEP_BUILDERS.get(step_name)
                if not builder:
                    raise ValueError(f"Unsupported step: {step_name}")

                params = methods.get(step_name, {})
                pipeline_steps.append((step_name, builder(method_name, params)))

            return Pipeline(pipeline_steps)

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def build(self):
        try:
            x_pipeline = self._build_pipeline("x")
            y_pipeline = self._build_pipeline("y")
            return x_pipeline, y_pipeline
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\data_processors\scaler_factory.py
================================================================================

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.pipeline import Pipeline

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class ScalerFactory:
    """
    Factory to create scaling transformers.
    Supports: standard, minmax, robust.
    Easily extendable with new scalers.
    """
    _SUPPORTED_METHODS = {
        "standard": StandardScaler,
        "minmax": MinMaxScaler,
        "robust": RobustScaler
    }

    @staticmethod
    def get_scaler_pipeline(method: str, params: dict = None) -> Pipeline:
        try:
            if method in ScalerFactory._SUPPORTED_METHODS:
                scaler_class = ScalerFactory._SUPPORTED_METHODS[method]
                scaler = scaler_class(**(params or {}))
            else:
                raise ValueError(f"Unsupported scaler method: {method}")

            return Pipeline([("scaler", scaler)])

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\dbhandler\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\dbhandler\base_handler.py
================================================================================

from abc import ABC, abstractmethod
from pathlib import Path
import pandas as pd

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class DBHandler(ABC):
    """
    Abstract base class for all database or storage handlers.
    Enables unified behavior across MongoDB, CSV, PostgreSQL, S3, etc.
    """

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    @abstractmethod
    def close(self) -> None:
        """
        Clean up resources like DB connections or sessions.
        """
        pass

    @abstractmethod
    def load_from_source(self) -> pd.DataFrame:
        """
        Load and return a DataFrame from the underlying data source.
        Example:
            - MongoDB: collection
            - PostgreSQL: table
            - CSVHandler: file
            - S3Handler: object
        """
        pass

    def load_from_csv(self, source: Path) -> pd.DataFrame:
        """
        Generic utility: load a DataFrame from a CSV file.
        Available to all subclasses.
        """
        try:
            df = pd.read_csv(source)
            logger.info(f"DataFrame loaded from CSV: {source}")
            return df
        except Exception as e:
            logger.error(f"Failed to load DataFrame from CSV: {source}")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\dbhandler\mongodb_handler.py
================================================================================

from pymongo import MongoClient
from pymongo.server_api import ServerApi
import pandas as pd
from pathlib import Path

from src.networksecurity.entity.config_entity import MongoHandlerConfig
from src.networksecurity.utils.core import csv_to_json_convertor, save_to_json
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from src.networksecurity.constants.constants import (
    MONGODB_CONNECT_TIMEOUT_MS,
    MONGODB_SOCKET_TIMEOUT_MS,
)

from src.networksecurity.dbhandler.base_handler import DBHandler


class MongoDBHandler(DBHandler):
    def __init__(self, config: MongoHandlerConfig):
        self.config = config
        self._client: MongoClient | None = None
        self._owns_client: bool = False

    def _get_client(self) -> MongoClient:
        if self._client is None:
            self._client = MongoClient(
                self.config.mongodb_uri,
                server_api=ServerApi("1"),
                connectTimeoutMS=MONGODB_CONNECT_TIMEOUT_MS,
                socketTimeoutMS=MONGODB_SOCKET_TIMEOUT_MS,
            )
            logger.info("MongoClient initialized.")
        return self._client

    def close(self) -> None:
        if self._client:
            self._client.close()
            logger.info("MongoClient connection closed.")
            self._client = None

    def ping_mongodb(self) -> None:
        try:
            self._get_client().admin.command("ping")
            logger.info("MongoDB ping successful.")
        except Exception as e:
            logger.error("MongoDB ping failed.")
            raise NetworkSecurityError(e, logger) from e

    def insert_csv_to_collection(self, csv_filepath: Path) -> int:
        try:
            records = csv_to_json_convertor(csv_filepath, self.config.json_data_filepath)

            # Save records as JSON using core utility
            save_to_json(records, self.config.json_data_filepath, label="Converted JSON Records")

            db = self._get_client()[self.config.database_name]
            collection = db[self.config.collection_name]
            result = collection.insert_many(records)
            logger.info(
                f"Inserted {len(result.inserted_ids)} records into "
                f"{self.config.database_name}.{self.config.collection_name}"
            )
            return len(result.inserted_ids)
        except Exception as e:
            logger.error("Failed to insert records into MongoDB.")
            raise NetworkSecurityError(e, logger) from e

    def load_from_source(self) -> pd.DataFrame:
        """
        Load data from the configured MongoDB collection as a pandas DataFrame.
        """
        try:
            db = self._get_client()[self.config.database_name]
            collection = db[self.config.collection_name]
            records = list(collection.find())
            df = pd.DataFrame(records)
            logger.info(
                f"Exported {len(df)} documents from "
                f"{self.config.database_name}.{self.config.collection_name} as DataFrame."
            )
            return df
        except Exception as e:
            logger.error("Failed to export data from MongoDB.")
            raise NetworkSecurityError(e, logger) from e
