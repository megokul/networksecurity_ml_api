
ğŸ“¦ Project Structure of: E:\MyProjects\networksecurity

ğŸ“ .dvc/
    ğŸ“„ .gitignore
    ğŸ“ cache/
        ğŸ“ files/
            ğŸ“ md5/
                ğŸ“ 68/
                    ğŸ“„ 770e0efceb9cd3777502f505ecbec4
                ğŸ“ 70/
                    ğŸ“„ 3506878baa6bf0360b2b42782e9677
                ğŸ“ 83/
                    ğŸ“„ de4e3a0dbe7d93403a8c15753377f7
                ğŸ“ 86/
                    ğŸ“„ 4e5c8474fcd1b15bc434932036628d
                ğŸ“ 87/
                    ğŸ“„ f220430be73d16609bfd637b1644af
                ğŸ“ 8d/
                    ğŸ“„ e71e17a20f71bece7659a34bae7027
                ğŸ“ bf/
                    ğŸ“„ fa4350e8aa16caaa3cbb92ec8e048c
                ğŸ“ e1/
                    ğŸ“„ a8ae356f072436dd3acc1bcbafbd89
    ğŸ“„ config
    ğŸ“ tmp/
        ğŸ“„ btime
        ğŸ“„ dag.md
        ğŸ“ exps/
            ğŸ“ cache/
                ğŸ“ 08/
                    ğŸ“„ ba3af380fc7743bb7784b0b99b9e84b597659c
                ğŸ“ 0c/
                    ğŸ“„ 7b66bd77f8e71f84c28f8868f79d1c391b243e
                ğŸ“ 17/
                    ğŸ“„ a15f92afbc3a237675719f1bb52d4e45b2855f
                ğŸ“ 20/
                    ğŸ“„ 418b499ec78701cefc3c979011ef77fa37084d
                ğŸ“ 28/
                    ğŸ“„ 1710634e34c59aaca7c823b82ba97e8c3ae6c8
                    ğŸ“„ e888b2e08a4b7dc34604b66abd2af2aaed6b86
                ğŸ“ 29/
                    ğŸ“„ 28b4f351c2d8f210468c693d53455c7b09a1eb
                    ğŸ“„ dfeb7151d1e0fc143507ecdecb1bedd2b5cc4d
                ğŸ“ 2e/
                    ğŸ“„ d0f21b70c06ff15b188bb69f6ce389a869af78
                ğŸ“ 32/
                    ğŸ“„ 9b746e5971f793e269ced861c2d1e401f43d48
                ğŸ“ 3e/
                    ğŸ“„ a84c7b28b0242174e45334d5d6a9d3026f546d
                ğŸ“ 48/
                    ğŸ“„ faf510268751ce441d9178a563f5e2c4577d88
                ğŸ“ 4a/
                    ğŸ“„ 640ce33b51ffeb8c1aa85e520ce78dfe105123
                ğŸ“ 4c/
                    ğŸ“„ 0bc699b6997f940ba6b39f9c3f002d74b0cd52
                ğŸ“ 51/
                    ğŸ“„ a289cbadb16786fe46c2b4bab48853efc637a9
                ğŸ“ 52/
                    ğŸ“„ 0a8ade47b7e748a46de7cd94c1f1db7a925527
                    ğŸ“„ b49095c9d6e6a69ba58f77d1979c4626e30907
                ğŸ“ 61/
                    ğŸ“„ a81d71c590a7a662176a82aa5f00a7532a3e16
                ğŸ“ 63/
                    ğŸ“„ 02ec043a14cb6881a1196ca1b58f7df093a586
                    ğŸ“„ b517b2eff95eb91aba4bfef87abcd7955ae01c
                ğŸ“ 64/
                    ğŸ“„ 3172fd8ac68c1fe35ae5f9a8b4734633a30778
                ğŸ“ 67/
                    ğŸ“„ fee62ed0bb527f9980ea136f0c312784242b38
                ğŸ“ 68/
                    ğŸ“„ 81874523aeb9b0b77eb7bae250d0287e341558
                ğŸ“ 6c/
                    ğŸ“„ 4d54822a8eec14b0c0ea4846ba14c9541e31dd
                ğŸ“ 7f/
                    ğŸ“„ e668c86a64d0171bc4dc8708460ac48ec77a34
                ğŸ“ 88/
                    ğŸ“„ a8f98863afa828023b2e3616348335b6d4628e
                ğŸ“ 89/
                    ğŸ“„ daed7059804441e9041b6d073c071a1cfcbe2c
                ğŸ“ 8a/
                    ğŸ“„ a79a390fba4858401f3e79d94bee5dbf354341
                    ğŸ“„ ed08439cb37bbee9fee4ad00d82ea30435606b
                ğŸ“ a5/
                    ğŸ“„ 168ff7e9460e593e744f60ef1bad04413e1ca7
                ğŸ“ a8/
                    ğŸ“„ 2c1cb5bbc048ca32cb78061a4b8aaf3735233f
                ğŸ“ ad/
                    ğŸ“„ 329b2130c11421d465202bdf74f5170e661c23
                ğŸ“ af/
                    ğŸ“„ ea20d43951b4448cd0a4eb4a7a134958cfb5c5
                ğŸ“ b0/
                    ğŸ“„ 63569211a95f11f23c4f61f6b9518bbf3b2cf3
                ğŸ“ b2/
                    ğŸ“„ a55eaf06589f37339db03046e528a4eb70f795
                ğŸ“ bb/
                    ğŸ“„ 6504b5b79c287b4b60d2355251d8b30025c769
                    ğŸ“„ e1aa1f0748e2a41f3ece907bfaaae1cafb5c48
                ğŸ“ c4/
                    ğŸ“„ 9033287ca54d77ad6ab9c7603c06825bbac4e6
                ğŸ“ c7/
                    ğŸ“„ 31a093cb23b7a2bef65922d39d9d20ac56e6f3
                ğŸ“ c8/
                    ğŸ“„ dbdb66a143e984e4fe8b43c7d755441cdec3f5
                ğŸ“ ce/
                    ğŸ“„ 1c26eef879f92adf044d9d0da37c028fd7d494
                    ğŸ“„ 4e08484f42af33f1129dda5084bf3fed271a22
                ğŸ“ d2/
                    ğŸ“„ 3e801f5cac5d3a86a0e23a6fdbe1cbe281f789
                ğŸ“ d6/
                    ğŸ“„ 4f71f9b0de7fb7fca7d5f21249de0a489b4ca8
                ğŸ“ dc/
                    ğŸ“„ fff6f4ba5cf77e9a0eaf75c0830d9795bb58cf
                ğŸ“ dd/
                    ğŸ“„ ee659d7114460f19e1804a98117314724c9b04
                ğŸ“ e3/
                    ğŸ“„ 5de325a2125360b5ae289c080fc748a0140cdc
                    ğŸ“„ 8f209c48f47e7a864c8c9c61048ce16f1a2896
                ğŸ“ eb/
                    ğŸ“„ 79e8cfe8c4a974d876650c1a48de56761ef2a1
                ğŸ“ f3/
                    ğŸ“„ 8d3509705265cc0399a9e13b577074bfb9aedf
                ğŸ“ f6/
                    ğŸ“„ f9d61771a2404a0d798efa45ff7ff15c8f639a
                ğŸ“ f9/
                    ğŸ“„ 21cb7aa3662a13f16a67e07a71781cec9b1a91
            ğŸ“ celery/
                ğŸ“ broker/
                    ğŸ“ control/
                    ğŸ“ in/
                    ğŸ“ processed/
                ğŸ“ result/
        ğŸ“„ lock
        ğŸ“„ rwlock
        ğŸ“„ rwlock.lock
ğŸ“„ .dvcignore
ğŸ“„ .env
ğŸ“„ .gitignore
ğŸ“„ Dockerfile
ğŸ“„ LICENSE
ğŸ“ NetworkSecurity.egg-info/
    ğŸ“„ PKG-INFO
    ğŸ“„ SOURCES.txt
    ğŸ“„ dependency_links.txt
    ğŸ“„ requires.txt
    ğŸ“„ top_level.txt
ğŸ“„ README.md
ğŸ“„ app.py
ğŸ“ artifacts/
    ğŸ“ 2025_05_24T09_51_21Z/
        ğŸ“ data_ingestion/
            ğŸ“ featurestore/
                ğŸ“„ raw_data.csv
            ğŸ“ ingested/
                ğŸ“„ ingested_data.csv
        ğŸ“ data_transformation/
            ğŸ“ preprocessor/
                ğŸ“„ x_preprocessor.joblib
                ğŸ“„ y_preprocessor.joblib
            ğŸ“ transformed/
                ğŸ“ test/
                    ğŸ“„ x_test.npy
                    ğŸ“„ y_test.npy
                ğŸ“ train/
                    ğŸ“„ x_train.npy
                    ğŸ“„ y_train.npy
                ğŸ“ val/
                    ğŸ“„ x_val.npy
                    ğŸ“„ y_val.npy
        ğŸ“ data_validation/
            ğŸ“ reports/
                ğŸ“„ drift_report.yaml
                ğŸ“„ duplicates_report.yaml
                ğŸ“„ missing_values_report.json
                ğŸ“„ validation_report.yaml
            ğŸ“ validated/
                ğŸ“„ validated_data.csv
        ğŸ“ model_evaluation/
            ğŸ“„ evaluation_report.yaml
        ğŸ“ model_trainer/
            ğŸ“ inference_model/
                ğŸ“„ inference_model.joblib
            ğŸ“ reports/
                ğŸ“„ training_report.yaml
            ğŸ“ trained_model/
                ğŸ“„ model.joblib
ğŸ“ config/
    ğŸ“„ config.yaml
    ğŸ“„ params.yaml
    ğŸ“„ schema.yaml
    ğŸ“„ templates.yaml
ğŸ“ data/
    ğŸ“ raw/
        ğŸ“„ raw_data.csv
        ğŸ“„ raw_data.csv.dvc
    ğŸ“ transformed/
        ğŸ“ test/
            ğŸ“„ x_test.npy
            ğŸ“„ x_test.npy.dvc
            ğŸ“„ y_test.npy
            ğŸ“„ y_test.npy.dvc
        ğŸ“ train/
            ğŸ“„ x_train.npy
            ğŸ“„ x_train.npy.dvc
            ğŸ“„ y_train.npy
            ğŸ“„ y_train.npy.dvc
        ğŸ“ val/
            ğŸ“„ x_val.npy
            ğŸ“„ x_val.npy.dvc
            ğŸ“„ y_val.npy
            ğŸ“„ y_val.npy.dvc
    ğŸ“ validated/
        ğŸ“„ validated_data.csv
        ğŸ“„ validated_data.csv.dvc
ğŸ“„ debug.py
ğŸ“„ debug_pipeline.py
ğŸ“„ docker-compose.yaml
ğŸ“ final_model/
    ğŸ“„ final_inference_model.joblib
ğŸ“ logs/
    ğŸ“ 2025_05_24T09_51_21Z/
        ğŸ“„ 2025_05_24T09_51_21Z.log
ğŸ“„ main.py
ğŸ“ network_data/
    ğŸ“ input_csv/
        ğŸ“„ phisingData.csv
        ğŸ“„ phisingData_check.csv
ğŸ“ prediction_output/
ğŸ“„ print_structure.py
ğŸ“„ project_dump.py
ğŸ“„ project_template.py
ğŸ“„ requirements.txt
ğŸ“ research/
    ğŸ“ config/
        ğŸ“„ schema.yaml
    ğŸ“„ ingested_data.csv
    ğŸ“„ research.ipynb
ğŸ“„ setup.py
ğŸ“ src/
    ğŸ“ networksecurity/
        ğŸ“„ __init__.py
        ğŸ“ components/
            ğŸ“„ __init__.py
            ğŸ“„ data_ingestion.py
            ğŸ“„ data_transformation.py
            ğŸ“„ data_validation.py
            ğŸ“„ model_evaluation.py
            ğŸ“„ model_pusher.py
            ğŸ“„ model_trainer.py
        ğŸ“ config/
            ğŸ“„ __init__.py
            ğŸ“„ configuration.py
        ğŸ“ constants/
            ğŸ“„ __init__.py
            ğŸ“„ constants.py
        ğŸ“ data_processors/
            ğŸ“„ encoder_factory.py
            ğŸ“„ imputer_factory.py
            ğŸ“„ label_mapper.py
            ğŸ“„ preprocessor_builder.py
            ğŸ“„ scaler_factory.py
        ğŸ“ dbhandler/
            ğŸ“„ __init__.py
            ğŸ“„ base_handler.py
            ğŸ“„ mongodb_handler.py
            ğŸ“„ s3_handler.py
        ğŸ“ entity/
            ğŸ“„ __init__.py
            ğŸ“„ artifact_entity.py
            ğŸ“„ config_entity.py
        ğŸ“ exception/
            ğŸ“„ __init__.py
            ğŸ“„ exception.py
        ğŸ“ inference/
            ğŸ“„ estimator.py
        ğŸ“ logging/
            ğŸ“„ __init__.py
            ğŸ“„ logger.py
        ğŸ“ pipeline/
            ğŸ“„ __init__.py
            ğŸ“„ data_ingestion_pipeline.py
            ğŸ“„ data_transformation_pipeline.py
            ğŸ“„ data_validation_pipeline.py
            ğŸ“„ model_evaluation_pipeline.py
            ğŸ“„ model_pusher_pipeline.py
            ğŸ“„ model_trainer_pipeline.py
            ğŸ“„ training_pipeline.py
        ğŸ“ utils/
            ğŸ“„ __init__.py
            ğŸ“„ core.py
            ğŸ“„ timestamp.py
        ğŸ“ worker/
            ğŸ“„ celery_worker.py
ğŸ“ templates/
    ğŸ“„ table.html

--- CODE DUMP | PART 4 of 4 ---


================================================================================
# PY FILE: src\networksecurity\pipeline\model_pusher_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_pusher import ModelPusher
from src.networksecurity.entity.artifact_entity import ModelTrainerArtifact, ModelPusherArtifact
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class ModelPusherPipeline:
    """
    Orchestrates the model pushing stage.

    Responsibilities:
    - Loads model pusher configuration
    - Accepts the ModelTrainerArtifact
    - Saves final model to local path
    - Pushes model to S3 via S3Syncer
    - Emits a ModelPusherArtifact
    """

    def __init__(self, model_trainer_artifact: ModelTrainerArtifact) -> None:
        try:
            logger.info("Initializing ModelPusherPipeline...")

            self.config = ConfigurationManager().get_model_pusher_config()
            self.model_trainer_artifact = model_trainer_artifact

        except Exception as e:
            logger.exception("Failed to initialize ModelPusherPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelPusherArtifact:
        try:
            logger.info("========== Model Pusher Stage Started ==========")

            model_pusher = ModelPusher(
                model_pusher_config=self.config,
                model_trainer_artifact=self.model_trainer_artifact
            )
            pusher_artifact = model_pusher.push_model()

            logger.info(f"Model Pusher Stage Completed Successfully: {pusher_artifact}")
            logger.info("========== Model Pusher Stage Completed ==========")

            return pusher_artifact

        except Exception as e:
            logger.exception("Model Pusher Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\model_trainer_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_trainer import ModelTrainer
from src.networksecurity.entity.artifact_entity import (
    DataTransformationArtifact,
    ModelTrainerArtifact
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class ModelTrainerPipeline:
    """
    Orchestrates the Model Training stage of the pipeline.

    Responsibilities:
    - Loads modelâ€trainer configuration
    - Accepts transformation artifact
    - Trains (and tunes) candidate models
    - Logs & registers via MLflow
    - Emits a ModelTrainerArtifact
    """

    def __init__(self, transformation_artifact: DataTransformationArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.config = self.config_manager.get_model_trainer_config()
            self.transformation_artifact = transformation_artifact
        except Exception as e:
            logger.exception("Failed to initialize ModelTrainerPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelTrainerArtifact:
        try:
            logger.info("========== Model Training Stage Started ==========")

            trainer = ModelTrainer(
                config=self.config,
                transformation_artifact=self.transformation_artifact
            )
            trainer_artifact = trainer.run_training()

            logger.info(f"Model Training Completed Successfully: {trainer_artifact}")
            logger.info("========== Model Training Stage Completed ==========")

            return trainer_artifact

        except Exception as e:
            logger.exception("Model Training Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\training_pipeline.py
================================================================================

import sys

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger

from src.networksecurity.components.data_ingestion import DataIngestion
from src.networksecurity.components.data_validation import DataValidation
from src.networksecurity.components.data_transformation import DataTransformation
from src.networksecurity.components.model_trainer import ModelTrainer
from src.networksecurity.components.model_evaluation import ModelEvaluation
from src.networksecurity.components.model_pusher import ModelPusher

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.dbhandler.mongodb_handler import MongoDBHandler
from src.networksecurity.dbhandler.s3_handler import S3Handler

from src.networksecurity.entity.artifact_entity import (
    DataIngestionArtifact,
    DataValidationArtifact,
    DataTransformationArtifact,
    ModelTrainerArtifact,
    ModelEvaluationArtifact,
    ModelPusherArtifact,
)


class TrainingPipeline:
    """
    Orchestrates the full end-to-end training pipeline:
    - MongoDB ingestion
    - Validation
    - Transformation
    - Training
    - Evaluation
    - Model push (local + optional S3)
    """

    def __init__(self) -> None:
        try:
            self.config_manager = ConfigurationManager()
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_pipeline(self) -> ModelPusherArtifact:
        try:
            logger.info("========== Training Pipeline Started ==========")

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # MongoDB Handler
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            mongo_config = self.config_manager.get_mongo_handler_config()
            mongo_handler = MongoDBHandler(config=mongo_config)

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Data Ingestion
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            ingestion_config = self.config_manager.get_data_ingestion_config()
            ingestion = DataIngestion(config=ingestion_config, db_handler=mongo_handler)
            ingestion_artifact: DataIngestionArtifact = ingestion.run_ingestion()

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Data Validation
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            validation_config = self.config_manager.get_data_validation_config()
            validation = DataValidation(config=validation_config, ingestion_artifact=ingestion_artifact)
            validation_artifact: DataValidationArtifact = validation.run_validation()

            if not validation_artifact.validation_status:
                logger.error("Validation failed. Aborting pipeline.")
                raise RuntimeError("Data validation failed. Pipeline terminated.")

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Data Transformation
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            transformation_config = self.config_manager.get_data_transformation_config()
            transformation = DataTransformation(config=transformation_config, validation_artifact=validation_artifact)
            transformation_artifact: DataTransformationArtifact = transformation.run_transformation()

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Model Training
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            trainer_config = self.config_manager.get_model_trainer_config()
            trainer = ModelTrainer(config=trainer_config, transformation_artifact=transformation_artifact)
            trainer_artifact: ModelTrainerArtifact = trainer.run_training()

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Model Evaluation
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            evaluation_config = self.config_manager.get_model_evaluation_config()
            evaluator = ModelEvaluation(config=evaluation_config, trainer_artifact=trainer_artifact)
            evaluation_artifact: ModelEvaluationArtifact = evaluator.run_evaluation()

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Model Pusher
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            pusher_config = self.config_manager.get_model_pusher_config()
            s3_handler_config = self.config_manager.get_s3_handler_config()
            s3_handler = S3Handler(config=s3_handler_config)

            pusher = ModelPusher(
                model_pusher_config=pusher_config,
                s3_handler=s3_handler,
                model_trainer_artifact=trainer_artifact
            )
            pusher_artifact: ModelPusherArtifact = pusher.push_model()

            logger.info("========== Training Pipeline Completed Successfully ==========")
            return pusher_artifact

        except Exception as e:
            logger.exception("Training Pipeline Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\utils\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\utils\core.py
================================================================================

import json
import joblib
import pandas as pd
import numpy as np
import yaml
from pathlib import Path
from box import ConfigBox
from box.exceptions import BoxKeyError, BoxTypeError, BoxValueError
from urllib.parse import quote_plus
from ensure import ensure_annotations

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


@ensure_annotations
def read_yaml(path_to_yaml: Path) -> ConfigBox:
    """
    Load a YAML file as a ConfigBox, always using UTF-8.
    """
    if not path_to_yaml.exists():
        msg = f"YAML file not found: '{path_to_yaml}'"
        logger.error(msg)
        raise NetworkSecurityError(FileNotFoundError(msg), logger)

    try:
        # Explicitly open as UTF-8 to avoid Windows cp1252 issues
        with path_to_yaml.open("r", encoding="utf-8") as f:
            content = yaml.safe_load(f)
    except (BoxValueError, BoxTypeError, BoxKeyError, yaml.YAMLError) as e:
        logger.error(f"Failed to load YAML from {path_to_yaml.as_posix()}: {e}")
        raise NetworkSecurityError(e, logger) from e
    except Exception as e:
        logger.error(f"Unexpected error reading YAML file: {e}")
        raise NetworkSecurityError(e, logger) from e

    if content is None:
        msg = "YAML file is empty or improperly formatted."
        logger.error(msg)
        raise NetworkSecurityError(ValueError(msg), logger)

    logger.info(f"YAML loaded successfully from: '{path_to_yaml.as_posix()}'")
    return ConfigBox(content)


@ensure_annotations
def csv_to_json_convertor(source_filepath: Path, destination_filepath: Path):
    """
    Convert a CSV file to a list of JSON records and optionally save it.
    """
    try:
        if not source_filepath.exists():
            raise FileNotFoundError(f"CSV source file not found: '{source_filepath.as_posix()}'")

        df = pd.read_csv(source_filepath).reset_index(drop=True)
        records = df.to_dict(orient="records")

        parent_dir = destination_filepath.parent
        if not parent_dir.exists():
            parent_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created directory for JSON output: '{parent_dir.as_posix()}'")
        else:
            logger.info(f"Directory already exists for JSON output: '{parent_dir.as_posix()}'")

        with destination_filepath.open("w", encoding="utf-8") as f:
            json.dump(records, f, indent=4)

        logger.info(f"CSV converted and saved to: '{destination_filepath.as_posix()}'")
        return records

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def replace_username_password_in_uri(base_uri: str, username: str, password: str) -> str:
    if not all([base_uri, username, password]):
        raise ValueError("base_uri, username, and password must all be provided.")

    encoded_username = quote_plus(username)
    encoded_password = quote_plus(password)

    return (
        base_uri
        .replace("<username>", encoded_username)
        .replace("<password>", encoded_password)
    )


@ensure_annotations
def save_to_yaml(data: dict, *paths: Path, label: str):
    """
    Write a dict out to YAML, always using UTF-8.
    """
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            # Write UTF-8
            with open(path, "w", encoding="utf-8") as file:
                yaml.dump(data, file, sort_keys=False)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def save_to_csv(df: pd.DataFrame, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            df.to_csv(path, index=False)
            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def save_array(array: np.ndarray | pd.Series, *paths: Path, label: str):
    """
    Saves a NumPy array or pandas Series to the specified paths in `.npy` format.

    Args:
        array (Union[np.ndarray, pd.Series]): Data to save.
        *paths (Path): One or more file paths.
        label (str): Label for logging.
    """
    try:
        # Convert Series to ndarray if needed
        array = np.asarray(array)

        for path in paths:
            path = Path(path)

            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            # # Ensure file ends with `.npy`
            # if path.suffix != ".npy":
            #     path = path.with_suffix(".npy")

            np.save(path, array)
            logger.info(f"{label} saved to: '{path.as_posix()}'")

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e
    
@ensure_annotations
def load_array(path: Path, label: str) -> np.ndarray:
    """
    Loads a NumPy array from the specified `.npy` file path.

    Args:
        path (Path): Path to the `.npy` file.
        label (str): Label for logging.

    Returns:
        np.ndarray: Loaded NumPy array.
    """
    try:
        path = Path(path)

        if not path.exists():
            raise FileNotFoundError(f"{label} file not found at path: '{path.as_posix()}'")

        array = np.load(path)
        logger.info(f"{label} loaded successfully from: '{path.as_posix()}'")
        return array

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def save_to_json(data: dict, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            with open(path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=4)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def read_csv(path: Path, label: str) -> pd.DataFrame:
    try:
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"{label} not found at path: {path.as_posix()}")

        df = pd.read_csv(path)
        logger.info(f"{label} loaded from: '{path.as_posix()}'")
        return df

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def save_object(obj: object, path: Path, label: str):
    try:
        path = Path(path)
        if not path.parent.exists():
            path.parent.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
        else:
            logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

        joblib.dump(obj, path)
        logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def load_object(path: Path):
    """
    Load a serialized Python object from the given path using joblib.

    Args:
        path (Path): Path to the saved object file.

    Returns:
        object: The deserialized Python object.

    Raises:
        NetworkSecurityError: If loading fails.
    """
    try:
        path = Path(path)
        logger.info(f"Loading object from: '{path.as_posix()}'")
        return joblib.load(path)
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\utils\timestamp.py
================================================================================

from datetime import datetime, timezone

# Internal module-level cache to ensure the timestamp is consistent across the entire pipeline
_timestamp_cache: str = None

def get_shared_utc_timestamp(fmt: str = "%Y_%m_%dT%H_%M_%SZ") -> str:
    """
    Returns a consistent, cached UTC timestamp for the current pipeline run.

    This function guarantees that:
    - The timestamp is generated only once.
    - All modules (logger, config, etc.) use the exact same value.
    - The format is clean and safe for filenames, directories, and versioning.

    Args:
        fmt (str): Optional datetime format string.
                   Default: 'YYYY_MM_DDTHH_MM_SSZ' (e.g., '2025_04_16T17_45_02Z')

    Returns:
        str: UTC timestamp string formatted per the provided format.
    """
    global _timestamp_cache
    if _timestamp_cache is None:
        _timestamp_cache = datetime.now(timezone.utc).strftime(fmt)
    return _timestamp_cache

================================================================================
# PY FILE: src\networksecurity\worker\celery_worker.py
================================================================================

from celery import Celery
from src.networksecurity.pipeline.training_pipeline import TrainingPipeline

celery_app = Celery(
    "networksecurity_tasks",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/0",
)

@celery_app.task(name="trigger_training_task")
def trigger_training_task():
    pipeline = TrainingPipeline()
    pipeline.run_pipeline()

================================================================================
# YAML FILE: config\config.yaml
================================================================================

# Root directory for all experiment artifacts
project:
  artifacts_root: artifacts

# MongoDB configuration
mongo_handler:
  input_data_path: network_data/input_csv/phisingData.csv
  json_data_filename: input_data.json
  database_name: network_security_db
  collection_name: phishing_records

# Data ingestion configuration
data_ingestion:
  raw_data_filename: raw_data.csv
  ingested_data_filename: ingested_data.csv

# Data validation configuration
data_validation:
  validated_filename: validated_data.csv
  missing_report_filename: missing_values_report.json
  duplicates_report_filename: duplicates_report.yaml
  drift_report_filename: drift_report.yaml
  validation_report_filename: validation_report.yaml
  schema_hash_filename: schema_hash.json


# Data transformation configuration
data_transformation:
  x_train_filename: x_train.npy
  y_train_filename: y_train.npy
  x_val_filename: x_val.npy
  y_val_filename: y_val.npy
  x_test_filename: x_test.npy
  y_test_filename: y_test.npy
  x_preprocessor_filename: x_preprocessor.joblib
  y_preprocessor_filename: y_preprocessor.joblib
  

# Model trainer configuration
model_trainer:
  model_dir: saved_models
  trained_model_filename: model.joblib
  # where to dump a short training report
  training_report_filename: training_report.yaml

# Model prediction configuration
model_prediction:
  prediction_output_filename: prediction_output.csv

# Stable DVC-tracked data paths
data_paths:
  raw_data_dvc_filepath: data/raw/raw_data.csv
  validated_dvc_filepath: data/validated/validated_data.csv
  train_dvc_dir: data/transformed/train
  val_dvc_dir: data/transformed/val
  test_dvc_dir: data/transformed/test

# Model evaluation configuration
model_evaluation:
  evaluation_report_filename: evaluation_report.yaml
  evaluated_model_filename: evaluated_model.joblib

model_pusher:
  final_model_filename: final_inference_model.joblib

s3_handler:
  final_model_s3_bucket: networksecurity-dev-artifacts
  s3_final_model_prefix: final_model
  s3_artifacts_prefix: artifacts

================================================================================
# YAML FILE: config\params.yaml
================================================================================

# Parameters for drift detection during data validation
validation_params:
  drift_detection:
    enabled: true
    method: ks_test
    p_value_threshold: 0.05

  schema_check:
    enabled: true
    method: hash


# Parameters for data transformation
transformation_params:
  # Parameters for data splitting
  data_split:
    train_size: 0.6
    test_size: 0.2
    val_size: 0.2
    random_state: 42
    stratify: true

  steps:
    x:
      imputer: knn
    y:
      label_mapping: label_mapper

  methods:
    x:
      imputer:
        missing_values: null
        n_neighbors: 3
        weights: uniform
    y:
      label_mapping:
        from: -1
        to: 0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parameters for model training
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parameters for model training
model_trainer:
  # candidate estimators (importable by sklearn convention)
  models:
    - name: sklearn.ensemble.RandomForestClassifier
      # default params (used if optimization.enabled = false)
      params:
        n_estimators: 100
        max_depth: 10
        random_state: 42
      # search-space for Optuna
      search_space:
        n_estimators:
          distribution: int
          low: 50
          high: 300
          step: 10
        max_depth:
          distribution: int
          low: 5
          high: 50
          step: 1

    - name: sklearn.ensemble.GradientBoostingClassifier
      params:
        n_estimators: 100
        learning_rate: 0.1
        max_depth: 3
        random_state: 42
      search_space:
        n_estimators:
          distribution: int
          low: 50
          high: 200
          step: 10
        learning_rate:
          distribution: float
          low: 0.01
          high: 1.0
          log: true
        max_depth:
          distribution: int
          low: 2
          high: 10

  # hyperparameter optimization settings
  optimization:
    enabled: true
    method: optuna
    n_trials: 30
    direction: maximize
    cv_folds: 5
    scoring: accuracy

  # MLflow tracking & registry settings (URI is now picked from ENV)
  tracking:
    mlflow:
      enabled: true
      experiment_name: NetworkSecurityExperiment
      registry_model_name: NetworkSecurityModel
      metrics_to_log:
        - accuracy
        - f1
        - precision
        - recall
      # new switch: whether to log each Optuna trial as its own MLflow run
      log_trials: false

model_pusher:
   upload_to_s3: true

================================================================================
# YAML FILE: config\schema.yaml
================================================================================

columns:
  Abnormal_URL: int64
  DNSRecord: int64
  Domain_registeration_length: int64
  Favicon: int64
  Google_Index: int64
  HTTPS_token: int64
  Iframe: int64
  Links_in_tags: int64
  Links_pointing_to_page: int64
  Page_Rank: int64
  Prefix_Suffix: int64
  Redirect: int64
  Request_URL: int64
  Result: int64
  RightClick: int64
  SFH: int64
  SSLfinal_State: int64
  Shortining_Service: int64
  Statistical_report: int64
  Submitting_to_email: int64
  URL_Length: int64
  URL_of_Anchor: int64
  age_of_domain: int64
  double_slash_redirecting: int64
  having_At_Symbol: int64
  having_IP_Address: int64
  having_Sub_Domain: int64
  on_mouseover: int64
  popUpWidnow: int64
  port: int64
  web_traffic: int64

target_column: Result

target_labels:
  0: phishing
  1: legitimate

================================================================================
# YAML FILE: config\templates.yaml
================================================================================

validation_report:
  # UTC timestamp of the validation run
  timestamp: ""
  
  # Overall validation pass/fail
  validation_status: null
  critical_passed: null
  non_critical_passed: null

  # Which schemaâ€check was used (â€œhashâ€ or â€œstructureâ€)
  schema_check_type: ""
  # Which driftâ€detection method (if any)
  drift_check_method: ""

  check_results:
    critical_checks:
      # True if the schema hash/structure matched
      schema_is_match: null
      # True if no data drift was detected
      no_data_drift: null

    non_critical_checks:
      # True if no missing values were found
      no_missing_values: null
      # True if no duplicate rows were found
      no_duplicate_rows: null


# templates.yaml

# â€¦ your existing templates â€¦

training_report:
  timestamp: ""
  best_model: ""
  best_model_params: {}
  train_metrics: {}
  val_metrics: {}
  optimization:
    method: ""
    best_trial: null
    n_trials: null
    direction: ""
    cv:
      folds: null
      mean_score: null
      std_score: null

================================================================================
# YAML FILE: research\config\schema.yaml
================================================================================

columns:
  Abnormal_URL: int64
  DNSRecord: int64
  Domain_registeration_length: int64
  Favicon: int64
  Google_Index: int64
  HTTPS_token: int64
  Iframe: int64
  Links_in_tags: int64
  Links_pointing_to_page: int64
  Page_Rank: int64
  Prefix_Suffix: int64
  Redirect: int64
  Request_URL: int64
  Result: int64
  RightClick: int64
  SFH: int64
  SSLfinal_State: int64
  Shortining_Service: int64
  Statistical_report: int64
  Submitting_to_email: int64
  URL_Length: int64
  URL_of_Anchor: int64
  age_of_domain: int64
  double_slash_redirecting: int64
  having_At_Symbol: int64
  having_IP_Address: int64
  having_Sub_Domain: int64
  on_mouseover: int64
  popUpWidnow: int64
  port: int64
  web_traffic: int64
